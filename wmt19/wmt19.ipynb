{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nightly PyTorch/XLA\n",
    "# %pip install --user https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torchvision-nightly-cp38-cp38-linux_x86_64.whl 'torch_xla[tpuvm] @ https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly-cp38-cp38-linux_x86_64.whl'\n",
    "# %pip install datasets transformers sacremoses evaluate sklearn sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune FairSeq Transformer pretrained on wmt19-de-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "FLAGS['batch_size'] = 4\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 5e-5\n",
    "FLAGS['num_epochs'] = 3\n",
    "FLAGS['num_cores'] = 8\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False\n",
    "FLAGS['source_lang'] = \"de\"\n",
    "FLAGS['target_lang'] = \"en\"\n",
    "FLAGS['metrics_debug'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer, get_scheduler\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import time\n",
    "\n",
    "model_name = \"facebook/wmt19-de-en\"\n",
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(FSMTForConditionalGeneration.from_pretrained(model_name))\n",
    "tokenizer = FSMTTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def finetune():\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  def get_dataset():\n",
    "    def preprocess(examples):\n",
    "      inputs = [ex[FLAGS['source_lang']] for ex in examples[\"translation\"]]\n",
    "      targets = [ex[FLAGS['target_lang']] for ex in examples[\"translation\"]]\n",
    "      return tokenizer(text=inputs, text_target=targets, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    ds = load_dataset(\"news_commentary\", \"de-en\", split=\"train\")\n",
    "    ds = ds.map(preprocess, batched=True)\n",
    "    ds = ds.remove_columns([\"id\", \"translation\"])\n",
    "    ds = ds.train_test_split(test_size=0.2)\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds[\"train\"].shuffle(seed=42).select(range(1000)), ds[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "  # Using the serial executor avoids multiple processes to\n",
    "  # download the same data.\n",
    "  small_train_dataset, small_test_dataset = SERIAL_EXEC.run(get_dataset)\n",
    "\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      small_train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      small_train_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      small_test_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      shuffle=False,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  \n",
    "  # Scale learning rate to world size\n",
    "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "\n",
    "  # Get optimizer, scheduler and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "  lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=FLAGS['num_epochs'] * len(train_loader))\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, batch in enumerate(loader):\n",
    "      optimizer.zero_grad()\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      lr_scheduler.step()\n",
    "      tracker.add(FLAGS['batch_size'])\n",
    "      if x % FLAGS['log_steps'] == 0:\n",
    "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "            tracker.global_rate(), time.asctime()), flush=True)\n",
    "  \n",
    "  def test_loop_fn(loader):\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "      \n",
    "      decoded_preds = [pred.strip() for pred in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "      decoded_labels = [[label.strip()] for label in tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)]\n",
    "      metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print('[xla:{}] Bleu={:.5f} Time={}'.format(\n",
    "            xm.get_ordinal(), eval_metric[\"score\"], time.asctime()), flush=True)\n",
    "  \n",
    "  # Train and eval loops\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    xm.master_print(\"Started training epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    xm.master_print(\"Evaluate epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_fn(rank, flags):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset news_commentary (/home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72b8fa7c2a2497c843be308b7921fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset news_commentary (/home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-e89eb8a3c0e3c6c5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-475817f4134776eb.arrow and /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-7a41dd2bddbbe0fb.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-9b1de6915cc54941.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-1dd426fa7ae9e23a.arrow\n",
      "WARNING:datasets.builder:Found cached dataset news_commentary (/home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-e89eb8a3c0e3c6c5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-475817f4134776eb.arrow and /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-7a41dd2bddbbe0fb.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-9b1de6915cc54941.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-1dd426fa7ae9e23a.arrow\n",
      "WARNING:datasets.builder:Found cached dataset news_commentary (/home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-e89eb8a3c0e3c6c5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-475817f4134776eb.arrow and /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-7a41dd2bddbbe0fb.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-9b1de6915cc54941.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-1dd426fa7ae9e23a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch 1\n",
      "[xla:1](0) Loss=12.07927 Rate=0.16 GlobalRate=0.16 Time=Tue Nov 22 19:02:52 2022\n",
      "[xla:3](0) Loss=12.36526 Rate=0.16 GlobalRate=0.16 Time=Tue Nov 22 19:02:53 2022\n",
      "[xla:2](0) Loss=12.23764 Rate=0.15 GlobalRate=0.15 Time=Tue Nov 22 19:02:56 2022\n",
      "[xla:0](0) Loss=12.01942 Rate=0.15 GlobalRate=0.15 Time=Tue Nov 22 19:02:59 2022\n",
      "[xla:2](20) Loss=3.34164 Rate=0.10 GlobalRate=0.07 Time=Tue Nov 22 19:23:02 2022\n",
      "[xla:3](20) Loss=3.29067 Rate=0.10 GlobalRate=0.07 Time=Tue Nov 22 19:23:02 2022\n",
      "[xla:1](20) Loss=3.37073 Rate=0.10 GlobalRate=0.07 Time=Tue Nov 22 19:23:02 2022\n",
      "[xla:0](20) Loss=3.30566 Rate=0.10 GlobalRate=0.07 Time=Tue Nov 22 19:23:02 2022\n",
      "[xla:2](40) Loss=0.20429 Rate=4.92 GlobalRate=0.13 Time=Tue Nov 22 19:23:12 2022\n",
      "[xla:0](40) Loss=0.24094 Rate=4.92 GlobalRate=0.13 Time=Tue Nov 22 19:23:12 2022\n",
      "[xla:1](40) Loss=0.12618 Rate=4.91 GlobalRate=0.13 Time=Tue Nov 22 19:23:12 2022\n",
      "[xla:3](40) Loss=0.25264 Rate=4.90 GlobalRate=0.13 Time=Tue Nov 22 19:23:12 2022\n",
      "[xla:0](60) Loss=0.13276 Rate=6.85 GlobalRate=0.20 Time=Tue Nov 22 19:23:22 2022\n",
      "[xla:1](60) Loss=0.18472 Rate=6.84 GlobalRate=0.19 Time=Tue Nov 22 19:23:22 2022\n",
      "[xla:2](60) Loss=0.14628 Rate=6.79 GlobalRate=0.19 Time=Tue Nov 22 19:23:22 2022\n",
      "[xla:3](60) Loss=0.22642 Rate=6.81 GlobalRate=0.19 Time=Tue Nov 22 19:23:22 2022\n",
      "Finished training epoch 1\n",
      "Evaluate epoch 1\n",
      "[xla:3] Bleu=4.10721 Time=Tue Nov 22 19:26:02 2022\n",
      "[xla:2] Bleu=4.10721 Time=Tue Nov 22 19:26:02 2022\n",
      "[xla:0] Bleu=4.10721 Time=Tue Nov 22 19:26:03 2022\n",
      "Started training epoch 2\n",
      "[xla:1] Bleu=4.10721 Time=Tue Nov 22 19:26:03 2022\n",
      "[xla:3](0) Loss=0.10039 Rate=2.14 GlobalRate=2.14 Time=Tue Nov 22 19:26:04 2022\n",
      "[xla:2](0) Loss=1.60092 Rate=1.56 GlobalRate=1.56 Time=Tue Nov 22 19:26:05 2022\n",
      "[xla:0](0) Loss=0.28281 Rate=1.61 GlobalRate=1.61 Time=Tue Nov 22 19:26:06 2022\n",
      "[xla:1](0) Loss=0.36698 Rate=1.48 GlobalRate=1.48 Time=Tue Nov 22 19:26:06 2022\n",
      "[xla:3](20) Loss=0.10199 Rate=4.64 GlobalRate=5.77 Time=Tue Nov 22 19:26:17 2022\n",
      "[xla:2](20) Loss=0.16064 Rate=4.48 GlobalRate=5.59 Time=Tue Nov 22 19:26:17 2022\n",
      "[xla:1](20) Loss=0.22068 Rate=4.88 GlobalRate=6.04 Time=Tue Nov 22 19:26:17 2022\n",
      "[xla:0](20) Loss=0.13810 Rate=4.90 GlobalRate=6.10 Time=Tue Nov 22 19:26:17 2022\n",
      "[xla:2](40) Loss=0.13865 Rate=6.32 GlobalRate=6.40 Time=Tue Nov 22 19:26:28 2022\n",
      "[xla:3](40) Loss=0.17996 Rate=6.36 GlobalRate=6.51 Time=Tue Nov 22 19:26:28 2022\n",
      "[xla:0](40) Loss=0.17731 Rate=6.48 GlobalRate=6.73 Time=Tue Nov 22 19:26:28 2022\n",
      "[xla:1](40) Loss=0.07224 Rate=6.46 GlobalRate=6.68 Time=Tue Nov 22 19:26:28 2022\n",
      "[xla:2](60) Loss=0.12054 Rate=7.17 GlobalRate=6.78 Time=Tue Nov 22 19:26:38 2022\n",
      "[xla:1](60) Loss=0.14531 Rate=7.23 GlobalRate=7.00 Time=Tue Nov 22 19:26:38 2022\n",
      "[xla:3](60) Loss=0.19494 Rate=7.18 GlobalRate=6.86 Time=Tue Nov 22 19:26:38 2022\n",
      "[xla:0](60) Loss=0.10897 Rate=7.23 GlobalRate=7.03 Time=Tue Nov 22 19:26:38 2022\n",
      "Finished training epoch 2\n",
      "Evaluate epoch 2\n",
      "[xla:3] Bleu=4.60650 Time=Tue Nov 22 19:28:09 2022\n",
      "[xla:1] Bleu=4.60650 Time=Tue Nov 22 19:28:10 2022\n",
      "[xla:0] Bleu=4.60650 Time=Tue Nov 22 19:28:11 2022\n",
      "Started training epoch 3\n",
      "[xla:3](0) Loss=0.08472 Rate=1.48 GlobalRate=1.48 Time=Tue Nov 22 19:28:12 2022\n",
      "[xla:2] Bleu=4.60650 Time=Tue Nov 22 19:28:12 2022\n",
      "[xla:1](0) Loss=0.32177 Rate=1.33 GlobalRate=1.33 Time=Tue Nov 22 19:28:13 2022\n",
      "[xla:2](0) Loss=1.39283 Rate=1.64 GlobalRate=1.64 Time=Tue Nov 22 19:28:14 2022\n",
      "[xla:0](0) Loss=0.24739 Rate=1.47 GlobalRate=1.47 Time=Tue Nov 22 19:28:14 2022\n",
      "[xla:2](20) Loss=0.13281 Rate=4.88 GlobalRate=6.09 Time=Tue Nov 22 19:28:25 2022\n",
      "[xla:3](20) Loss=0.09156 Rate=4.06 GlobalRate=5.08 Time=Tue Nov 22 19:28:25 2022[xla:1](20) Loss=0.20018 Rate=4.33 GlobalRate=5.37 Time=Tue Nov 22 19:28:25 2022\n",
      "\n",
      "[xla:0](20) Loss=0.12027 Rate=4.84 GlobalRate=6.00 Time=Tue Nov 22 19:28:25 2022\n",
      "[xla:2](40) Loss=0.12313 Rate=6.56 GlobalRate=6.77 Time=Tue Nov 22 19:28:36 2022\n",
      "[xla:3](40) Loss=0.17050 Rate=6.22 GlobalRate=6.08 Time=Tue Nov 22 19:28:36 2022\n",
      "[xla:0](40) Loss=0.17437 Rate=6.53 GlobalRate=6.71 Time=Tue Nov 22 19:28:36 2022\n",
      "[xla:1](40) Loss=0.06649 Rate=6.31 GlobalRate=6.28 Time=Tue Nov 22 19:28:36 2022\n",
      "[xla:2](60) Loss=0.11129 Rate=7.48 GlobalRate=7.15 Time=Tue Nov 22 19:28:46 2022\n",
      "[xla:3](60) Loss=0.18964 Rate=7.36 GlobalRate=6.62 Time=Tue Nov 22 19:28:46 2022\n",
      "[xla:1](60) Loss=0.13873 Rate=7.41 GlobalRate=6.79 Time=Tue Nov 22 19:28:46 2022\n",
      "[xla:0](60) Loss=0.10332 Rate=7.48 GlobalRate=7.11 Time=Tue Nov 22 19:28:46 2022\n",
      "Finished training epoch 3\n",
      "Evaluate epoch 3\n",
      "[xla:2] Bleu=4.75591 Time=Tue Nov 22 19:37:17 2022\n",
      "[xla:0] Bleu=4.75591 Time=Tue Nov 22 19:37:18 2022\n",
      "[xla:1] Bleu=4.75591 Time=Tue Nov 22 19:37:19 2022\n",
      "[xla:3] Bleu=4.75591 Time=Tue Nov 22 19:37:19 2022\n"
     ]
    }
   ],
   "source": [
    "xmp.spawn(mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
