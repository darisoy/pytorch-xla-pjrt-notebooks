{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune FairSeq Transformer pretrained on wmt19-de-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer, get_scheduler\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import time\n",
    "\n",
    "model_name = \"facebook/wmt19-de-en\"\n",
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(FSMTForConditionalGeneration.from_pretrained(model_name))\n",
    "tokenizer = FSMTTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def finetune():\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  def get_dataset():\n",
    "    def preprocess(examples):\n",
    "      inputs = [ex[FLAGS['source_lang']] for ex in examples[\"translation\"]]\n",
    "      targets = [ex[FLAGS['target_lang']] for ex in examples[\"translation\"]]\n",
    "      return tokenizer(text=inputs, text_target=targets, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    ds = load_dataset(\"news_commentary\", \"de-en\", split=\"train\")\n",
    "    ds = ds.map(preprocess, batched=True)\n",
    "    ds = ds.remove_columns([\"id\", \"translation\"])\n",
    "    ds = ds.train_test_split(test_size=0.2)\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds[\"train\"].shuffle(seed=42).select(range(1000)), ds[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "  # Using the serial executor avoids multiple processes to\n",
    "  # download the same data.\n",
    "  small_train_dataset, small_test_dataset = SERIAL_EXEC.run(get_dataset)\n",
    "\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      small_train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      small_train_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      small_test_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      shuffle=False,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  \n",
    "  # Scale learning rate to world size\n",
    "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "\n",
    "  # Get optimizer, scheduler and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "  lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=FLAGS['num_epochs'] * len(train_loader))\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, batch in enumerate(loader):\n",
    "      optimizer.zero_grad()\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      lr_scheduler.step()\n",
    "      tracker.add(FLAGS['batch_size'])\n",
    "      if x % FLAGS['log_steps'] == 0:\n",
    "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "            tracker.global_rate(), time.asctime()), flush=True)\n",
    "  \n",
    "  def test_loop_fn(loader):\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "      \n",
    "      decoded_preds = [pred.strip() for pred in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "      decoded_labels = [[label.strip()] for label in tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)]\n",
    "      metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print('[xla:{}] Bleu={:.5f} Time={}'.format(\n",
    "            xm.get_ordinal(), eval_metric[\"score\"], time.asctime()), flush=True)\n",
    "  \n",
    "  # Train and eval loops\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    xm.master_print(\"Started training epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    xm.master_print(\"Evaluate epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_fn(rank, flags):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  finetune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
