{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune FairSeq Transformer pretrained on wmt19-de-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import FSMTForConditionalGeneration, FSMTTokenizer, get_scheduler\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.optim import AdamW\n",
    "# import evaluate\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.debug.metrics as met\n",
    "# import torch_xla.distributed.parallel_loader as pl\n",
    "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "# import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained tokenizer and model\n",
    "# mname = \"facebook/wmt19-de-en\"\n",
    "# tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "# model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darisoy/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is great, isn't it?\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# input = \"Maschinelles Lernen ist gro√üartig, oder?\"\n",
    "# input_ids = tokenizer.encode(input, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "# outputs = model.generate(input_ids)\n",
    "# decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(decoded) # \"Machine learning is great, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_lang = \"de\"\n",
    "# target_lang = \"en\"\n",
    "\n",
    "# def preprocess(examples):\n",
    "#   inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "#   targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "#   return tokenizer(text=inputs, text_target=targets, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset news_commentary (/home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/darisoy/.cache/huggingface/datasets/news_commentary/de-en/11.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4/cache-2e11ed8504aaeb10.arrow\n"
     ]
    }
   ],
   "source": [
    "# Get and preprocess the data\n",
    "# ds = load_dataset(\"news_commentary\", \"de-en\", split=\"train\")\n",
    "# ds = ds.map(preprocess, batched=True)\n",
    "# ds = ds.remove_columns([\"id\", \"translation\"])\n",
    "# ds = ds.train_test_split(test_size=0.2)\n",
    "# ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = ds[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = ds[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "# eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#   print('epoch {} begin'.format(epoch), flush=True)\n",
    "#   for x, batch in enumerate(train_dataloader):\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     outputs = model(**batch)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     if x % 100 == 0:\n",
    "#       print('epoch {}: Step = {} Loss={:.5f}'.format(epoch, x, loss.item()), flush=True)\n",
    "#   print('epoch {} end'.format(epoch), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# model.eval()\n",
    "# for batch in eval_dataloader:\n",
    "#   batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#   with torch.no_grad():\n",
    "#     outputs = model(**batch)\n",
    "\n",
    "#   logits = outputs.logits\n",
    "#   predictions = torch.argmax(logits, dim=-1)\n",
    "#   metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer, get_scheduler\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import time\n",
    "\n",
    "model_name = \"facebook/wmt19-de-en\"\n",
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(FSMTForConditionalGeneration.from_pretrained(model_name))\n",
    "tokenizer = FSMTTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def finetune():\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  def get_dataset():\n",
    "    def preprocess(examples):\n",
    "      inputs = [ex[FLAGS['source_lang']] for ex in examples[\"translation\"]]\n",
    "      targets = [ex[FLAGS['target_lang']] for ex in examples[\"translation\"]]\n",
    "      return tokenizer(text=inputs, text_target=targets, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    ds = load_dataset(\"news_commentary\", \"de-en\", split=\"train\")\n",
    "    ds = ds.map(preprocess, batched=True)\n",
    "    ds = ds.remove_columns([\"id\", \"translation\"])\n",
    "    ds = ds.train_test_split(test_size=0.2)\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds[\"train\"].shuffle(seed=42).select(range(1000)), ds[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "  # Using the serial executor avoids multiple processes to\n",
    "  # download the same data.\n",
    "  small_train_dataset, small_test_dataset = SERIAL_EXEC.run(get_dataset)\n",
    "\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      small_train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      small_train_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      small_test_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      shuffle=False,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  \n",
    "  # Scale learning rate to world size\n",
    "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "\n",
    "  # Get optimizer, scheduler and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "  lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=FLAGS['num_epochs'] * len(train_loader))\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, batch in enumerate(loader):\n",
    "      optimizer.zero_grad()\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      lr_scheduler.step()\n",
    "      tracker.add(FLAGS['batch_size'])\n",
    "      if x % FLAGS['log_steps'] == 0:\n",
    "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "            tracker.global_rate(), time.asctime()), flush=True)\n",
    "  \n",
    "  def test_loop_fn(loader):\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "      \n",
    "      decoded_preds = [pred.strip() for pred in tokenizer.batch_decode(predictions, skip_special_tokens=True)]\n",
    "      decoded_labels = [[label.strip()] for label in tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)]\n",
    "      metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print('[xla:{}] Bleu={:.5f} Time={}'.format(\n",
    "            xm.get_ordinal(), eval_metric[\"score\"], time.asctime()), flush=True)\n",
    "  \n",
    "  # Train and eval loops\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    xm.master_print(\"Started training epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    xm.master_print(\"Evaluate epoch {}\".format(epoch))\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_fn(rank, flags):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  finetune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
