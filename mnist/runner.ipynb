{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nightly PyTorch/XLA\n",
    "# %pip install --user https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torchvision-nightly-cp38-cp38-linux_x86_64.whl 'torch_xla[tpuvm] @ https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly-cp38-cp38-linux_x86_64.whl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "DO NOT access the TPU on this file. ex dont:\n",
    "`xm.get_xla_supported_devices(\"TPU\")`\n",
    "\n",
    "if the TPU is accessed and this file can't run, kill the TPU processes:\n",
    "`$ lsof -t /dev/accel* | xargs kill`\n",
    "\n",
    "#### Why?\n",
    "For now `start_method='fork'` parameter of `xmp.spawn` isn't implemented for PjRt so `xmp.spawn` amd `mp_fn` functions have to live in seperate notebook files. This [StackOverflow answer](https://stackoverflow.com/a/42383397/13272853) explains the fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.mnist import mp_fn #pip install ipynb\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import os\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "FLAGS['datadir'] = \"/tmp/mnist\"\n",
    "FLAGS['batch_size'] = 128\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 0.01\n",
    "FLAGS['momentum'] = 0.5\n",
    "FLAGS['num_epochs'] = 10\n",
    "FLAGS['num_cores'] = 8\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xla:2](0) Loss=2.30512 Rate=52.24 GlobalRate=52.24 Time=Thu Nov 17 18:04:23 2022\n",
      "[xla:1](0) Loss=2.35747 Rate=55.04 GlobalRate=55.04 Time=Thu Nov 17 18:04:23 2022\n",
      "[xla:0](0) Loss=2.35175 Rate=54.30 GlobalRate=54.30 Time=Thu Nov 17 18:04:23 2022\n",
      "[xla:3](0) Loss=2.35390 Rate=55.07 GlobalRate=55.07 Time=Thu Nov 17 18:04:23 2022\n",
      "[xla:1](20) Loss=1.78930 Rate=155.69 GlobalRate=194.56 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:0](20) Loss=1.75310 Rate=155.52 GlobalRate=194.26 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:3](20) Loss=1.75169 Rate=155.98 GlobalRate=194.91 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:2](20) Loss=1.92788 Rate=153.07 GlobalRate=191.02 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:2](40) Loss=1.18057 Rate=19814.68 GlobalRate=370.90 Time=Thu Nov 17 18:04:34 2022[xla:1](40) Loss=1.15255 Rate=19440.23 GlobalRate=377.68 Time=Thu Nov 17 18:04:34 2022\n",
      "\n",
      "[xla:3](40) Loss=1.26643 Rate=19419.67 GlobalRate=378.36 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:0](40) Loss=1.22893 Rate=18223.62 GlobalRate=376.97 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:1](60) Loss=0.67611 Rate=24087.33 GlobalRate=558.14 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:2](60) Loss=0.68231 Rate=24223.70 GlobalRate=548.17 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:3](60) Loss=0.66933 Rate=24109.56 GlobalRate=559.14 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:0](60) Loss=0.58023 Rate=23711.37 GlobalRate=557.12 Time=Thu Nov 17 18:04:34 2022\n",
      "[xla:0](80) Loss=0.39648 Rate=24449.14 GlobalRate=734.40 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:3](80) Loss=0.46700 Rate=23962.13 GlobalRate=736.80 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:2](80) Loss=0.59440 Rate=23989.96 GlobalRate=722.46 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:1](80) Loss=0.38240 Rate=22267.39 GlobalRate=734.75 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:3](100) Loss=0.40550 Rate=22056.93 GlobalRate=910.76 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:0](100) Loss=0.32492 Rate=22229.37 GlobalRate=907.80 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:2](100) Loss=0.41337 Rate=22049.06 GlobalRate=893.16 Time=Thu Nov 17 18:04:35 2022\n",
      "[xla:1](100) Loss=0.39865 Rate=21495.20 GlobalRate=908.31 Time=Thu Nov 17 18:04:35 2022\n",
      "Finished training epoch 1\n",
      "[xla:2] Accuracy=92.26%\n",
      "[xla:1] Accuracy=92.29%\n",
      "[xla:0] Accuracy=92.91%\n",
      "[xla:3] Accuracy=91.18%\n",
      "[xla:2](0) Loss=0.29549 Rate=71.42 GlobalRate=71.42 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:1](0) Loss=0.36396 Rate=77.60 GlobalRate=77.60 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:0](0) Loss=0.28459 Rate=73.96 GlobalRate=73.96 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:3](0) Loss=0.34930 Rate=73.22 GlobalRate=73.22 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:0](20) Loss=0.24730 Rate=8009.97 GlobalRate=1397.79 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:2](20) Loss=0.28676 Rate=5217.61 GlobalRate=1287.27 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:1](20) Loss=0.25957 Rate=5585.77 GlobalRate=1395.68 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:3](20) Loss=0.14920 Rate=18154.88 GlobalRate=1466.59 Time=Thu Nov 17 18:04:40 2022\n",
      "[xla:2](40) Loss=0.21952 Rate=19853.10 GlobalRate=2413.34 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:1](40) Loss=0.24033 Rate=20024.26 GlobalRate=2607.99 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:0](40) Loss=0.30464 Rate=20903.42 GlobalRate=2611.18 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:3](40) Loss=0.34243 Rate=24744.34 GlobalRate=2732.37 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:2](60) Loss=0.20576 Rate=23926.68 GlobalRate=3438.63 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:0](60) Loss=0.10669 Rate=24364.47 GlobalRate=3707.85 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:1](60) Loss=0.22378 Rate=23983.68 GlobalRate=3703.22 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:3](60) Loss=0.21213 Rate=25508.99 GlobalRate=3867.13 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:2](80) Loss=0.22855 Rate=25649.95 GlobalRate=4381.72 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:3](80) Loss=0.19503 Rate=26524.30 GlobalRate=4906.34 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:0](80) Loss=0.16258 Rate=24281.55 GlobalRate=4688.28 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:1](80) Loss=0.10954 Rate=23918.18 GlobalRate=4679.42 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:2](100) Loss=0.17986 Rate=26290.80 GlobalRate=5250.99 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:3](100) Loss=0.15655 Rate=26699.96 GlobalRate=5853.36 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:1](100) Loss=0.21345 Rate=25868.20 GlobalRate=5596.81 Time=Thu Nov 17 18:04:41 2022\n",
      "[xla:0](100) Loss=0.13055 Rate=25657.82 GlobalRate=5601.87 Time=Thu Nov 17 18:04:41 2022\n",
      "Finished training epoch 2\n",
      "[xla:2] Accuracy=95.45%\n",
      "[xla:1] Accuracy=95.31%\n",
      "[xla:3] Accuracy=94.50%\n",
      "[xla:0] Accuracy=95.76%\n",
      "[xla:2](0) Loss=0.15203 Rate=73.28 GlobalRate=73.28 Time=Thu Nov 17 18:04:45 2022\n",
      "[xla:1](0) Loss=0.21560 Rate=74.54 GlobalRate=74.54 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:3](0) Loss=0.19131 Rate=74.11 GlobalRate=74.11 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:0](0) Loss=0.14014 Rate=73.37 GlobalRate=73.37 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:2](20) Loss=0.16571 Rate=7238.87 GlobalRate=1371.63 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:3](20) Loss=0.07191 Rate=13135.94 GlobalRate=1457.41 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:1](20) Loss=0.14396 Rate=10605.48 GlobalRate=1443.22 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:0](20) Loss=0.13097 Rate=17746.64 GlobalRate=1467.91 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:2](40) Loss=0.14206 Rate=20469.93 GlobalRate=2563.62 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:3](40) Loss=0.22648 Rate=22820.46 GlobalRate=2716.63 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:1](40) Loss=0.15251 Rate=21810.20 GlobalRate=2691.37 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:0](40) Loss=0.18780 Rate=24511.20 GlobalRate=2734.20 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:1](60) Loss=0.15641 Rate=26344.36 GlobalRate=3832.88 Time=Thu Nov 17 18:04:46 2022[xla:2](60) Loss=0.13463 Rate=25778.09 GlobalRate=3658.12 Time=Thu Nov 17 18:04:46 2022\n",
      "\n",
      "[xla:3](60) Loss=0.14011 Rate=26729.21 GlobalRate=3867.12 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:0](60) Loss=0.05301 Rate=27175.24 GlobalRate=3888.80 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:2](80) Loss=0.13353 Rate=26171.41 GlobalRate=4646.67 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:3](80) Loss=0.13741 Rate=26509.03 GlobalRate=4899.38 Time=Thu Nov 17 18:04:46 2022[xla:1](80) Loss=0.06217 Rate=26353.57 GlobalRate=4857.96 Time=Thu Nov 17 18:04:46 2022\n",
      "\n",
      "[xla:0](80) Loss=0.11302 Rate=26683.09 GlobalRate=4925.53 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:2](100) Loss=0.12044 Rate=26793.28 GlobalRate=5559.55 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:1](100) Loss=0.13071 Rate=26889.42 GlobalRate=5802.02 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:3](100) Loss=0.08038 Rate=26930.19 GlobalRate=5849.07 Time=Thu Nov 17 18:04:46 2022\n",
      "[xla:0](100) Loss=0.07434 Rate=26783.63 GlobalRate=5875.58 Time=Thu Nov 17 18:04:46 2022\n",
      "Finished training epoch 3\n",
      "[xla:2] Accuracy=96.63%\n",
      "[xla:0] Accuracy=96.69%\n",
      "[xla:1] Accuracy=96.35%\n",
      "[xla:3] Accuracy=96.00%\n",
      "[xla:2](0) Loss=0.11920 Rate=73.68 GlobalRate=73.68 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:1](0) Loss=0.15091 Rate=77.56 GlobalRate=77.56 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](0) Loss=0.14109 Rate=77.48 GlobalRate=77.48 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](0) Loss=0.09010 Rate=72.87 GlobalRate=72.87 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](20) Loss=0.04939 Rate=12191.49 GlobalRate=1511.49 Time=Thu Nov 17 18:04:51 2022[xla:2](20) Loss=0.12212 Rate=7407.67 GlobalRate=1381.77 Time=Thu Nov 17 18:04:51 2022\n",
      "\n",
      "[xla:1](20) Loss=0.10337 Rate=9700.93 GlobalRate=1485.78 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](20) Loss=0.08269 Rate=16020.86 GlobalRate=1451.01 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:2](40) Loss=0.10797 Rate=18749.83 GlobalRate=2569.24 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](40) Loss=0.17486 Rate=20660.19 GlobalRate=2797.91 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:1](40) Loss=0.11639 Rate=19678.94 GlobalRate=2752.86 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](40) Loss=0.13885 Rate=22678.19 GlobalRate=2695.55 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:2](60) Loss=0.10142 Rate=24119.09 GlobalRate=3657.06 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](60) Loss=0.10586 Rate=24870.85 GlobalRate=3967.12 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:1](60) Loss=0.12096 Rate=24458.01 GlobalRate=3905.98 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](60) Loss=0.04316 Rate=25637.87 GlobalRate=3828.15 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](80) Loss=0.10654 Rate=26179.16 GlobalRate=5026.14 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:2](80) Loss=0.09532 Rate=25855.88 GlobalRate=4649.71 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:1](80) Loss=0.04524 Rate=26007.32 GlobalRate=4952.09 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](80) Loss=0.08829 Rate=26496.48 GlobalRate=4858.02 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:3](100) Loss=0.05197 Rate=26628.97 GlobalRate=5991.06 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:1](100) Loss=0.09074 Rate=26593.45 GlobalRate=5907.17 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:2](100) Loss=0.09357 Rate=26489.82 GlobalRate=5560.57 Time=Thu Nov 17 18:04:51 2022\n",
      "[xla:0](100) Loss=0.05064 Rate=26708.47 GlobalRate=5798.48 Time=Thu Nov 17 18:04:51 2022\n",
      "Finished training epoch 4\n",
      "[xla:3] Accuracy=96.60%\n",
      "[xla:0] Accuracy=97.16%\n",
      "[xla:2] Accuracy=97.17%\n",
      "[xla:1] Accuracy=96.81%\n",
      "[xla:3](0) Loss=0.11192 Rate=71.23 GlobalRate=71.23 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:0](0) Loss=0.06266 Rate=72.41 GlobalRate=72.41 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:2](0) Loss=0.10556 Rate=71.49 GlobalRate=71.49 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:1](0) Loss=0.11243 Rate=69.41 GlobalRate=69.41 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:0](20) Loss=0.05973 Rate=5494.95 GlobalRate=1312.11 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:3](20) Loss=0.03755 Rate=5461.27 GlobalRate=1292.41 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:2](20) Loss=0.09778 Rate=6764.45 GlobalRate=1331.72 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:1](20) Loss=0.07787 Rate=16580.53 GlobalRate=1387.87 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:0](40) Loss=0.11459 Rate=18008.34 GlobalRate=2445.75 Time=Thu Nov 17 18:04:56 2022[xla:2](40) Loss=0.08890 Rate=18535.64 GlobalRate=2480.77 Time=Thu Nov 17 18:04:56 2022\n",
      "\n",
      "[xla:3](40) Loss=0.14479 Rate=17983.74 GlobalRate=2410.60 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:1](40) Loss=0.09634 Rate=22814.19 GlobalRate=2583.05 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:0](60) Loss=0.04029 Rate=23953.38 GlobalRate=3489.66 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:3](60) Loss=0.08609 Rate=23949.59 GlobalRate=3441.59 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:2](60) Loss=0.08116 Rate=24114.45 GlobalRate=3537.12 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:1](60) Loss=0.09690 Rate=25610.63 GlobalRate=3674.56 Time=Thu Nov 17 18:04:56 2022\n",
      "[xla:0](80) Loss=0.07126 Rate=25563.02 GlobalRate=4442.97 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:3](80) Loss=0.08655 Rate=25554.53 GlobalRate=4384.17 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:2](80) Loss=0.07849 Rate=25641.18 GlobalRate=4501.03 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:1](80) Loss=0.03360 Rate=26468.65 GlobalRate=4671.21 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:0](100) Loss=0.04032 Rate=26669.60 GlobalRate=5326.78 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:2](100) Loss=0.07549 Rate=26733.33 GlobalRate=5394.09 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:3](100) Loss=0.03719 Rate=26650.31 GlobalRate=5258.78 Time=Thu Nov 17 18:04:57 2022\n",
      "[xla:1](100) Loss=0.07106 Rate=26570.80 GlobalRate=5582.87 Time=Thu Nov 17 18:04:57 2022\n",
      "Finished training epoch 5\n",
      "[xla:3] Accuracy=97.07%\n",
      "[xla:0] Accuracy=97.47%\n",
      "[xla:1] Accuracy=97.17%\n",
      "[xla:2] Accuracy=97.59%\n",
      "[xla:0](0) Loss=0.04613 Rate=76.15 GlobalRate=76.15 Time=Thu Nov 17 18:05:01 2022\n",
      "[xla:3](0) Loss=0.08924 Rate=71.63 GlobalRate=71.63 Time=Thu Nov 17 18:05:01 2022\n",
      "[xla:1](0) Loss=0.08967 Rate=71.45 GlobalRate=71.45 Time=Thu Nov 17 18:05:01 2022\n",
      "[xla:2](0) Loss=0.09463 Rate=69.85 GlobalRate=69.85 Time=Thu Nov 17 18:05:01 2022\n",
      "[xla:1](20) Loss=0.06297 Rate=10694.66 GlobalRate=1388.88 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:0](20) Loss=0.04562 Rate=5758.35 GlobalRate=1379.19 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:3](20) Loss=0.03001 Rate=7075.43 GlobalRate=1340.64 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:2](20) Loss=0.08396 Rate=16755.19 GlobalRate=1396.76 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:1](40) Loss=0.08320 Rate=19141.98 GlobalRate=2574.18 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:3](40) Loss=0.12721 Rate=17718.40 GlobalRate=2489.36 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:0](40) Loss=0.09903 Rate=17180.54 GlobalRate=2557.23 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:2](40) Loss=0.07482 Rate=21667.95 GlobalRate=2588.94 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:0](60) Loss=0.03883 Rate=19965.00 GlobalRate=3598.93 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:3](60) Loss=0.07250 Rate=20171.40 GlobalRate=3508.32 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:1](60) Loss=0.08258 Rate=20719.80 GlobalRate=3621.03 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:2](60) Loss=0.07014 Rate=21320.30 GlobalRate=3634.20 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:0](80) Loss=0.06090 Rate=23900.24 GlobalRate=4575.36 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:3](80) Loss=0.07380 Rate=23988.23 GlobalRate=4465.01 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:1](80) Loss=0.02500 Rate=24217.94 GlobalRate=4602.46 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:2](80) Loss=0.06872 Rate=24675.22 GlobalRate=4621.13 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:0](100) Loss=0.03261 Rate=25696.12 GlobalRate=5475.08 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:1](100) Loss=0.05742 Rate=25835.94 GlobalRate=5506.36 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:3](100) Loss=0.02848 Rate=25722.18 GlobalRate=5348.12 Time=Thu Nov 17 18:05:02 2022\n",
      "[xla:2](100) Loss=0.06436 Rate=25863.58 GlobalRate=5525.63 Time=Thu Nov 17 18:05:02 2022\n",
      "Finished training epoch 6\n",
      "[xla:3] Accuracy=97.35%\n",
      "[xla:1] Accuracy=97.37%\n",
      "[xla:0] Accuracy=97.60%\n",
      "[xla:2] Accuracy=97.66%\n",
      "[xla:3](0) Loss=0.06894 Rate=74.64 GlobalRate=74.64 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:1](0) Loss=0.07462 Rate=74.39 GlobalRate=74.39 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:2](0) Loss=0.08610 Rate=74.47 GlobalRate=74.47 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](0) Loss=0.03603 Rate=71.24 GlobalRate=71.24 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:2](20) Loss=0.07303 Rate=13226.77 GlobalRate=1464.77 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:1](20) Loss=0.05176 Rate=8134.34 GlobalRate=1407.22 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:3](20) Loss=0.02364 Rate=5553.27 GlobalRate=1348.68 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](20) Loss=0.03828 Rate=15993.18 GlobalRate=1420.03 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:1](40) Loss=0.07325 Rate=14716.66 GlobalRate=2567.33 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:2](40) Loss=0.06635 Rate=16739.32 GlobalRate=2664.94 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:3](40) Loss=0.11424 Rate=13685.24 GlobalRate=2467.26 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](40) Loss=0.09110 Rate=17651.36 GlobalRate=2585.99 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:3](60) Loss=0.06414 Rate=20810.71 GlobalRate=3505.74 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:2](60) Loss=0.06437 Rate=22012.54 GlobalRate=3772.79 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:1](60) Loss=0.07471 Rate=21180.13 GlobalRate=3640.79 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](60) Loss=0.03570 Rate=22452.52 GlobalRate=3667.12 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:2](80) Loss=0.06337 Rate=24128.90 GlobalRate=4778.35 Time=Thu Nov 17 18:05:07 2022[xla:1](80) Loss=0.01904 Rate=23805.71 GlobalRate=4618.77 Time=Thu Nov 17 18:05:07 2022\n",
      "\n",
      "[xla:3](80) Loss=0.06337 Rate=23638.88 GlobalRate=4454.57 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](80) Loss=0.05088 Rate=24456.62 GlobalRate=4652.57 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:3](100) Loss=0.02342 Rate=24905.85 GlobalRate=5326.93 Time=Thu Nov 17 18:05:07 2022[xla:1](100) Loss=0.04799 Rate=24961.07 GlobalRate=5514.79 Time=Thu Nov 17 18:05:07 2022\n",
      "\n",
      "[xla:2](100) Loss=0.05693 Rate=25089.05 GlobalRate=5696.95 Time=Thu Nov 17 18:05:07 2022\n",
      "[xla:0](100) Loss=0.02686 Rate=25230.36 GlobalRate=5553.56 Time=Thu Nov 17 18:05:07 2022\n",
      "Finished training epoch 7\n",
      "[xla:0] Accuracy=97.79%\n",
      "[xla:1] Accuracy=97.62%\n",
      "[xla:3] Accuracy=97.56%\n",
      "[xla:2] Accuracy=97.83%\n",
      "[xla:0](0) Loss=0.03140 Rate=70.47 GlobalRate=70.47 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:2](0) Loss=0.07798 Rate=75.80 GlobalRate=75.80 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:1](0) Loss=0.06543 Rate=74.29 GlobalRate=74.29 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:3](0) Loss=0.05512 Rate=65.59 GlobalRate=65.59 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:1](20) Loss=0.04445 Rate=4787.56 GlobalRate=1313.93 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:0](20) Loss=0.03356 Rate=4420.18 GlobalRate=1240.93 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:2](20) Loss=0.06514 Rate=4650.62 GlobalRate=1330.00 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:3](20) Loss=0.02009 Rate=15657.79 GlobalRate=1311.33 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:0](40) Loss=0.08490 Rate=18038.82 GlobalRate=2321.59 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:1](40) Loss=0.06617 Rate=18162.75 GlobalRate=2451.99 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:2](40) Loss=0.05881 Rate=18146.21 GlobalRate=2480.91 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:3](40) Loss=0.10388 Rate=22489.11 GlobalRate=2447.21 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:0](60) Loss=0.03362 Rate=23127.37 GlobalRate=3312.62 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:2](60) Loss=0.05834 Rate=23173.51 GlobalRate=3530.06 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:1](60) Loss=0.06842 Rate=23144.39 GlobalRate=3490.33 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:3](60) Loss=0.05792 Rate=25389.82 GlobalRate=3488.55 Time=Thu Nov 17 18:05:12 2022\n",
      "[xla:0](80) Loss=0.04384 Rate=25344.40 GlobalRate=4227.53 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:2](80) Loss=0.05981 Rate=25394.12 GlobalRate=4493.92 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:1](80) Loss=0.01466 Rate=25390.62 GlobalRate=4445.50 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:3](80) Loss=0.05669 Rate=26047.91 GlobalRate=4440.58 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:0](100) Loss=0.02280 Rate=26843.37 GlobalRate=5080.89 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:2](100) Loss=0.05120 Rate=26848.58 GlobalRate=5388.59 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:1](100) Loss=0.03962 Rate=26838.11 GlobalRate=5332.58 Time=Thu Nov 17 18:05:13 2022\n",
      "[xla:3](100) Loss=0.01990 Rate=26820.22 GlobalRate=5323.49 Time=Thu Nov 17 18:05:13 2022\n",
      "Finished training epoch 8\n",
      "[xla:1] Accuracy=97.73%\n",
      "[xla:2] Accuracy=97.97%\n",
      "[xla:3] Accuracy=97.62%\n",
      "[xla:0] Accuracy=97.85%\n",
      "[xla:1](0) Loss=0.05894 Rate=74.86 GlobalRate=74.86 Time=Thu Nov 17 18:05:17 2022\n",
      "[xla:3](0) Loss=0.04529 Rate=73.36 GlobalRate=73.36 Time=Thu Nov 17 18:05:17 2022\n",
      "[xla:0](0) Loss=0.02824 Rate=73.38 GlobalRate=73.38 Time=Thu Nov 17 18:05:17 2022\n",
      "[xla:2](0) Loss=0.07228 Rate=69.82 GlobalRate=69.82 Time=Thu Nov 17 18:05:17 2022\n",
      "[xla:3](20) Loss=0.01785 Rate=11112.23 GlobalRate=1427.14 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:0](20) Loss=0.02992 Rate=16598.12 GlobalRate=1463.25 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:1](20) Loss=0.03842 Rate=3573.65 GlobalRate=1254.08 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:2](20) Loss=0.05927 Rate=16519.04 GlobalRate=1395.27 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:1](40) Loss=0.05924 Rate=17909.27 GlobalRate=2346.42 Time=Thu Nov 17 18:05:18 2022[xla:0](40) Loss=0.07943 Rate=23089.26 GlobalRate=2718.63 Time=Thu Nov 17 18:05:18 2022\n",
      "\n",
      "[xla:3](40) Loss=0.09519 Rate=20828.31 GlobalRate=2654.21 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:2](40) Loss=0.05354 Rate=22847.60 GlobalRate=2596.61 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:3](60) Loss=0.05306 Rate=24775.50 GlobalRate=3770.80 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:0](60) Loss=0.03027 Rate=25627.75 GlobalRate=3857.54 Time=Thu Nov 17 18:05:18 2022[xla:1](60) Loss=0.06329 Rate=23558.21 GlobalRate=3350.65 Time=Thu Nov 17 18:05:18 2022\n",
      "\n",
      "[xla:2](60) Loss=0.05436 Rate=25787.84 GlobalRate=3694.60 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:3](80) Loss=0.05074 Rate=26838.72 GlobalRate=4796.94 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:0](80) Loss=0.03982 Rate=26892.50 GlobalRate=4898.91 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:1](80) Loss=0.01203 Rate=25569.74 GlobalRate=4274.72 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:2](80) Loss=0.05845 Rate=26703.43 GlobalRate=4697.61 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:0](100) Loss=0.01919 Rate=26746.52 GlobalRate=5843.29 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:3](100) Loss=0.01796 Rate=26414.59 GlobalRate=5722.02 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:1](100) Loss=0.03459 Rate=26503.70 GlobalRate=5130.57 Time=Thu Nov 17 18:05:18 2022\n",
      "[xla:2](100) Loss=0.04534 Rate=26528.17 GlobalRate=5611.10 Time=Thu Nov 17 18:05:18 2022\n",
      "Finished training epoch 9\n",
      "[xla:2] Accuracy=98.06%\n",
      "[xla:1] Accuracy=97.83%\n",
      "[xla:0] Accuracy=97.99%\n",
      "[xla:3] Accuracy=97.70%\n",
      "[xla:2](0) Loss=0.06566 Rate=72.89 GlobalRate=72.89 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:0](0) Loss=0.02635 Rate=75.34 GlobalRate=75.34 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:1](0) Loss=0.05388 Rate=74.21 GlobalRate=74.21 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:3](0) Loss=0.03837 Rate=67.01 GlobalRate=67.01 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:1](20) Loss=0.03415 Rate=4371.71 GlobalRate=1293.22 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:0](20) Loss=0.02733 Rate=4370.23 GlobalRate=1309.40 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:2](20) Loss=0.05530 Rate=3775.56 GlobalRate=1240.93 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:3](20) Loss=0.01603 Rate=16672.85 GlobalRate=1342.43 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:2](40) Loss=0.04973 Rate=17743.95 GlobalRate=2321.38 Time=Thu Nov 17 18:05:23 2022[xla:1](40) Loss=0.05387 Rate=17962.18 GlobalRate=2414.80 Time=Thu Nov 17 18:05:23 2022\n",
      "\n",
      "[xla:0](40) Loss=0.07599 Rate=17924.30 GlobalRate=2443.43 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:3](40) Loss=0.08973 Rate=23348.88 GlobalRate=2505.69 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:0](60) Loss=0.02789 Rate=23771.24 GlobalRate=3485.23 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:1](60) Loss=0.05801 Rate=23726.71 GlobalRate=3445.53 Time=Thu Nov 17 18:05:23 2022[xla:2](60) Loss=0.04950 Rate=23639.16 GlobalRate=3317.50 Time=Thu Nov 17 18:05:23 2022\n",
      "\n",
      "[xla:3](60) Loss=0.04922 Rate=25498.52 GlobalRate=3566.13 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:2](80) Loss=0.05583 Rate=26604.21 GlobalRate=4243.70 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:1](80) Loss=0.01067 Rate=26630.56 GlobalRate=4401.17 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:0](80) Loss=0.03580 Rate=26628.65 GlobalRate=4449.72 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:3](80) Loss=0.04601 Rate=27360.93 GlobalRate=4549.38 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:0](100) Loss=0.01687 Rate=27461.11 GlobalRate=5339.05 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:1](100) Loss=0.02931 Rate=27452.41 GlobalRate=5282.85 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:2](100) Loss=0.04130 Rate=27415.80 GlobalRate=5100.36 Time=Thu Nov 17 18:05:23 2022\n",
      "[xla:3](100) Loss=0.01692 Rate=27493.82 GlobalRate=5450.70 Time=Thu Nov 17 18:05:23 2022\n",
      "Finished training epoch 10\n",
      "[xla:3] Accuracy=97.83%\n",
      "[xla:1] Accuracy=97.95%\n",
      "[xla:2] Accuracy=98.09%\n",
      "[xla:0] Accuracy=98.10%\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "xmp.spawn(mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
