{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nightly PyTorch/XLA\n",
    "# %pip install --user https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torchvision-nightly-cp38-cp38-linux_x86_64.whl 'torch_xla[tpuvm] @ https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly-cp38-cp38-linux_x86_64.whl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "DO NOT access the TPU on this file. ex dont:\n",
    "`xm.get_xla_supported_devices(\"TPU\")`\n",
    "\n",
    "if the TPU is accessed and this file can't run, kill the TPU processes:\n",
    "`$ lsof -t /dev/accel* | xargs kill`\n",
    "\n",
    "#### Why?\n",
    "For now `start_method='fork'` parameter of `xmp.spawn` isn't implemented for PjRt so `xmp.spawn` amd `mp_fn` functions have to live in seperate notebook files. This [StackOverflow answer](https://stackoverflow.com/a/42383397/13272853) explains the fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.mnist import mp_fn #pip install ipynb\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import os\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "FLAGS['datadir'] = \"/tmp/mnist\"\n",
    "FLAGS['batch_size'] = 128\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 0.01\n",
    "FLAGS['momentum'] = 0.5\n",
    "FLAGS['num_epochs'] = 10\n",
    "FLAGS['num_cores'] = 8\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xla:0](0) Loss=2.28643 Rate=50.40 GlobalRate=50.40 Time=Wed Nov 16 22:46:35 2022\n",
      "[xla:2](0) Loss=2.38794 Rate=51.50 GlobalRate=51.50 Time=Wed Nov 16 22:46:35 2022\n",
      "[xla:3](0) Loss=2.28669 Rate=53.38 GlobalRate=53.38 Time=Wed Nov 16 22:46:35 2022\n",
      "[xla:1](0) Loss=2.31845 Rate=52.18 GlobalRate=52.17 Time=Wed Nov 16 22:46:35 2022\n",
      "[xla:3](20) Loss=1.81443 Rate=151.67 GlobalRate=189.51 Time=Wed Nov 16 22:46:46 2022[xla:0](20) Loss=1.78182 Rate=150.26 GlobalRate=187.37 Time=Wed Nov 16 22:46:46 2022\n",
      "\n",
      "[xla:1](20) Loss=1.79712 Rate=151.75 GlobalRate=189.44 Time=Wed Nov 16 22:46:46 2022\n",
      "[xla:2](20) Loss=1.80213 Rate=150.73 GlobalRate=188.12 Time=Wed Nov 16 22:46:46 2022\n",
      "[xla:0](40) Loss=1.29402 Rate=21484.38 GlobalRate=364.00 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:2](40) Loss=1.16039 Rate=22485.51 GlobalRate=365.53 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:1](40) Loss=1.20555 Rate=21403.92 GlobalRate=368.00 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:3](40) Loss=1.31629 Rate=20911.04 GlobalRate=368.08 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:0](60) Loss=0.62869 Rate=27970.53 GlobalRate=538.60 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:3](60) Loss=0.65934 Rate=28198.63 GlobalRate=544.67 Time=Wed Nov 16 22:46:47 2022[xla:1](60) Loss=0.73184 Rate=27977.36 GlobalRate=544.49 Time=Wed Nov 16 22:46:47 2022\n",
      "\n",
      "[xla:2](60) Loss=0.69757 Rate=25699.21 GlobalRate=540.38 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:1](80) Loss=0.38866 Rate=27264.96 GlobalRate=718.23 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:0](80) Loss=0.45508 Rate=27232.97 GlobalRate=710.50 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:3](80) Loss=0.43740 Rate=27322.41 GlobalRate=718.46 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:2](80) Loss=0.54830 Rate=26722.06 GlobalRate=712.95 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:0](100) Loss=0.32433 Rate=27457.40 GlobalRate=880.34 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:1](100) Loss=0.34519 Rate=27452.20 GlobalRate=889.84 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:3](100) Loss=0.37988 Rate=27311.59 GlobalRate=890.07 Time=Wed Nov 16 22:46:47 2022\n",
      "[xla:2](100) Loss=0.38362 Rate=26680.25 GlobalRate=883.15 Time=Wed Nov 16 22:46:47 2022\n",
      "Finished training epoch 1\n",
      "[xla:0] Accuracy=92.58%\n",
      "[xla:1] Accuracy=92.29%\n",
      "[xla:3] Accuracy=92.78%\n",
      "[xla:2] Accuracy=92.53%\n",
      "[xla:0](0) Loss=0.28134 Rate=69.46 GlobalRate=69.46 Time=Wed Nov 16 22:46:52 2022\n",
      "[xla:1](0) Loss=0.40216 Rate=70.56 GlobalRate=70.56 Time=Wed Nov 16 22:46:52 2022\n",
      "[xla:3](0) Loss=0.31538 Rate=71.86 GlobalRate=71.86 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](0) Loss=0.34099 Rate=71.41 GlobalRate=71.41 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:3](20) Loss=0.15560 Rate=9706.63 GlobalRate=1385.58 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:0](20) Loss=0.22985 Rate=4668.84 GlobalRate=1236.61 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:1](20) Loss=0.26673 Rate=5122.96 GlobalRate=1270.47 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](20) Loss=0.30360 Rate=15947.96 GlobalRate=1422.92 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:3](40) Loss=0.23647 Rate=19912.60 GlobalRate=2577.85 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:1](40) Loss=0.23295 Rate=18010.27 GlobalRate=2372.81 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:0](40) Loss=0.34415 Rate=17759.28 GlobalRate=2311.55 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](40) Loss=0.24624 Rate=22810.78 GlobalRate=2647.09 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:3](60) Loss=0.18017 Rate=24272.89 GlobalRate=3665.73 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:0](60) Loss=0.15096 Rate=23549.44 GlobalRate=3303.25 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:1](60) Loss=0.22436 Rate=23540.66 GlobalRate=3386.32 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](60) Loss=0.19275 Rate=25402.54 GlobalRate=3759.43 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:3](80) Loss=0.18883 Rate=26124.05 GlobalRate=4662.77 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:0](80) Loss=0.19736 Rate=25823.18 GlobalRate=4219.14 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:1](80) Loss=0.12349 Rate=25796.39 GlobalRate=4320.87 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](80) Loss=0.20675 Rate=26770.11 GlobalRate=4779.22 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:1](100) Loss=0.16889 Rate=26758.71 GlobalRate=5185.83 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:3](100) Loss=0.14318 Rate=26811.48 GlobalRate=5578.55 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:0](100) Loss=0.14217 Rate=26713.23 GlobalRate=5067.57 Time=Wed Nov 16 22:46:53 2022\n",
      "[xla:2](100) Loss=0.18683 Rate=26505.95 GlobalRate=5703.65 Time=Wed Nov 16 22:46:53 2022\n",
      "Finished training epoch 2\n",
      "[xla:1] Accuracy=95.22%\n",
      "[xla:3] Accuracy=95.26%\n",
      "[xla:0] Accuracy=95.41%\n",
      "[xla:2] Accuracy=95.31%\n",
      "[xla:2](0) Loss=0.19218 Rate=73.89 GlobalRate=73.89 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:0](0) Loss=0.14409 Rate=72.07 GlobalRate=72.07 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](0) Loss=0.16027 Rate=71.17 GlobalRate=71.17 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:1](0) Loss=0.26233 Rate=70.21 GlobalRate=70.21 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](20) Loss=0.07606 Rate=15655.93 GlobalRate=1417.15 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:2](20) Loss=0.17799 Rate=10132.99 GlobalRate=1426.58 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:0](20) Loss=0.13547 Rate=14237.46 GlobalRate=1426.71 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:1](20) Loss=0.13820 Rate=14932.09 GlobalRate=1395.45 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](40) Loss=0.17470 Rate=21118.64 GlobalRate=2623.80 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:0](40) Loss=0.22680 Rate=20569.47 GlobalRate=2640.74 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:2](40) Loss=0.17401 Rate=18897.62 GlobalRate=2640.24 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:1](40) Loss=0.14837 Rate=21053.83 GlobalRate=2587.63 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](60) Loss=0.13350 Rate=23908.29 GlobalRate=3718.98 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:2](60) Loss=0.15613 Rate=23044.93 GlobalRate=3741.46 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:0](60) Loss=0.09020 Rate=23692.29 GlobalRate=3741.89 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:1](60) Loss=0.14053 Rate=23924.52 GlobalRate=3670.57 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](80) Loss=0.13488 Rate=24839.09 GlobalRate=4712.62 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:2](80) Loss=0.13885 Rate=24458.91 GlobalRate=4739.30 Time=Wed Nov 16 22:46:58 2022[xla:0](80) Loss=0.13017 Rate=24732.90 GlobalRate=4740.03 Time=Wed Nov 16 22:46:58 2022\n",
      "\n",
      "[xla:1](80) Loss=0.07475 Rate=24870.25 GlobalRate=4654.39 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:2](100) Loss=0.13386 Rate=24678.54 GlobalRate=5643.47 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:3](100) Loss=0.07888 Rate=24783.45 GlobalRate=5612.33 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:0](100) Loss=0.09490 Rate=24777.34 GlobalRate=5644.12 Time=Wed Nov 16 22:46:58 2022\n",
      "[xla:1](100) Loss=0.11753 Rate=24794.54 GlobalRate=5546.02 Time=Wed Nov 16 22:46:58 2022\n",
      "Finished training epoch 3\n",
      "[xla:2] Accuracy=96.45%\n",
      "[xla:3] Accuracy=96.26%\n",
      "[xla:0] Accuracy=96.40%\n",
      "[xla:1] Accuracy=96.31%\n",
      "[xla:3](0) Loss=0.10364 Rate=70.42 GlobalRate=70.42 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:2](0) Loss=0.15681 Rate=69.70 GlobalRate=69.70 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:0](0) Loss=0.09950 Rate=71.04 GlobalRate=71.04 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:1](0) Loss=0.19049 Rate=68.79 GlobalRate=68.79 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:3](20) Loss=0.05444 Rate=9541.32 GlobalRate=1358.18 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:2](20) Loss=0.13762 Rate=10022.44 GlobalRate=1350.74 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:0](20) Loss=0.09663 Rate=10656.15 GlobalRate=1381.00 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:1](20) Loss=0.08500 Rate=16050.72 GlobalRate=1373.75 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:2](40) Loss=0.14499 Rate=19918.15 GlobalRate=2515.14 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:0](40) Loss=0.17599 Rate=20169.12 GlobalRate=2568.80 Time=Wed Nov 16 22:47:03 2022[xla:3](40) Loss=0.14360 Rate=19713.14 GlobalRate=2528.26 Time=Wed Nov 16 22:47:03 2022\n",
      "\n",
      "[xla:1](40) Loss=0.11082 Rate=22697.00 GlobalRate=2558.68 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:3](60) Loss=0.10103 Rate=24239.39 GlobalRate=3598.72 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:0](60) Loss=0.06659 Rate=24413.20 GlobalRate=3653.81 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:2](60) Loss=0.13988 Rate=24260.18 GlobalRate=3580.28 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:1](60) Loss=0.10547 Rate=25388.51 GlobalRate=3639.70 Time=Wed Nov 16 22:47:03 2022\n",
      "[xla:3](80) Loss=0.10064 Rate=26252.77 GlobalRate=4582.68 Time=Wed Nov 16 22:47:04 2022\n",
      "[xla:0](80) Loss=0.09305 Rate=26320.68 GlobalRate=4649.89 Time=Wed Nov 16 22:47:04 2022\n",
      "[xla:2](80) Loss=0.10416 Rate=26296.57 GlobalRate=4560.55 Time=Wed Nov 16 22:47:04 2022\n",
      "[xla:1](80) Loss=0.05226 Rate=26301.25 GlobalRate=4627.82 Time=Wed Nov 16 22:47:04 2022\n",
      "[xla:3](100) Loss=0.05249 Rate=26767.99 GlobalRate=5485.27 Time=Wed Nov 16 22:47:04 2022[xla:0](100) Loss=0.07247 Rate=26803.25 GlobalRate=5562.56 Time=Wed Nov 16 22:47:04 2022\n",
      "[xla:2](100) Loss=0.10418 Rate=26802.41 GlobalRate=5460.07 Time=Wed Nov 16 22:47:04 2022\n",
      "\n",
      "[xla:1](100) Loss=0.08706 Rate=26957.38 GlobalRate=5539.43 Time=Wed Nov 16 22:47:04 2022\n",
      "Finished training epoch 4\n",
      "[xla:0] Accuracy=97.00%\n",
      "[xla:3] Accuracy=96.75%\n",
      "[xla:2] Accuracy=96.97%\n",
      "[xla:1] Accuracy=96.76%\n",
      "[xla:0](0) Loss=0.07980 Rate=69.16 GlobalRate=69.16 Time=Wed Nov 16 22:47:08 2022\n",
      "[xla:3](0) Loss=0.07744 Rate=68.84 GlobalRate=68.84 Time=Wed Nov 16 22:47:08 2022\n",
      "[xla:1](0) Loss=0.14991 Rate=71.25 GlobalRate=71.25 Time=Wed Nov 16 22:47:08 2022\n",
      "[xla:2](0) Loss=0.13551 Rate=67.63 GlobalRate=67.63 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:1](20) Loss=0.05580 Rate=10465.10 GlobalRate=1382.99 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:0](20) Loss=0.07262 Rate=4480.32 GlobalRate=1224.22 Time=Wed Nov 16 22:47:09 2022[xla:3](20) Loss=0.04267 Rate=7132.74 GlobalRate=1295.14 Time=Wed Nov 16 22:47:09 2022\n",
      "\n",
      "[xla:2](20) Loss=0.11298 Rate=16016.30 GlobalRate=1351.57 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:1](40) Loss=0.09282 Rate=19721.51 GlobalRate=2569.43 Time=Wed Nov 16 22:47:09 2022[xla:0](40) Loss=0.14657 Rate=17367.75 GlobalRate=2287.41 Time=Wed Nov 16 22:47:09 2022\n",
      "\n",
      "[xla:3](40) Loss=0.12800 Rate=18425.33 GlobalRate=2413.88 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:2](40) Loss=0.12339 Rate=22149.11 GlobalRate=2515.38 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:3](60) Loss=0.07931 Rate=23343.28 GlobalRate=3439.26 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:0](60) Loss=0.05321 Rate=22905.45 GlobalRate=3266.19 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:1](60) Loss=0.08830 Rate=23837.04 GlobalRate=3650.66 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:2](60) Loss=0.13072 Rate=24961.22 GlobalRate=3578.77 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:1](80) Loss=0.03741 Rate=25567.24 GlobalRate=4639.77 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:0](80) Loss=0.06996 Rate=25179.69 GlobalRate=4169.81 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:3](80) Loss=0.07980 Rate=25310.35 GlobalRate=4381.31 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:2](80) Loss=0.08066 Rate=25792.44 GlobalRate=4549.52 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:3](100) Loss=0.03733 Rate=25679.90 GlobalRate=5244.29 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:0](100) Loss=0.05845 Rate=25584.38 GlobalRate=5000.27 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:1](100) Loss=0.07002 Rate=25731.59 GlobalRate=5539.79 Time=Wed Nov 16 22:47:09 2022\n",
      "[xla:2](100) Loss=0.08042 Rate=25330.21 GlobalRate=5429.12 Time=Wed Nov 16 22:47:09 2022\n",
      "Finished training epoch 5\n",
      "[xla:3] Accuracy=97.12%\n",
      "[xla:0] Accuracy=97.42%\n",
      "[xla:2] Accuracy=97.32%\n",
      "[xla:1] Accuracy=97.26%\n",
      "[xla:3](0) Loss=0.06163 Rate=67.60 GlobalRate=67.60 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](0) Loss=0.06858 Rate=72.43 GlobalRate=72.43 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:1](0) Loss=0.12148 Rate=73.22 GlobalRate=73.22 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](0) Loss=0.11761 Rate=71.76 GlobalRate=71.76 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](20) Loss=0.05593 Rate=7142.30 GlobalRate=1355.43 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:1](20) Loss=0.04018 Rate=14603.95 GlobalRate=1450.21 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:3](20) Loss=0.03563 Rate=5870.97 GlobalRate=1246.51 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](20) Loss=0.09479 Rate=16689.12 GlobalRate=1432.88 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](40) Loss=0.12610 Rate=19711.74 GlobalRate=2530.05 Time=Wed Nov 16 22:47:14 2022[xla:3](40) Loss=0.11951 Rate=19229.22 GlobalRate=2335.14 Time=Wed Nov 16 22:47:14 2022\n",
      "\n",
      "[xla:1](40) Loss=0.08196 Rate=22572.22 GlobalRate=2697.73 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](40) Loss=0.10505 Rate=23488.81 GlobalRate=2667.62 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](60) Loss=0.04449 Rate=24535.51 GlobalRate=3603.95 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:3](60) Loss=0.06433 Rate=24333.60 GlobalRate=3337.18 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:1](60) Loss=0.08101 Rate=25796.85 GlobalRate=3833.20 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](60) Loss=0.12011 Rate=26216.06 GlobalRate=3792.85 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](80) Loss=0.05480 Rate=26689.38 GlobalRate=4592.62 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:1](80) Loss=0.02955 Rate=27191.39 GlobalRate=4872.24 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:3](80) Loss=0.06496 Rate=26579.36 GlobalRate=4265.13 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](80) Loss=0.06434 Rate=27487.13 GlobalRate=4824.66 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:0](100) Loss=0.05174 Rate=28101.58 GlobalRate=5511.40 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:1](100) Loss=0.05693 Rate=28294.94 GlobalRate=5833.53 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:3](100) Loss=0.03048 Rate=28069.13 GlobalRate=5132.27 Time=Wed Nov 16 22:47:14 2022\n",
      "[xla:2](100) Loss=0.06513 Rate=28096.33 GlobalRate=5774.58 Time=Wed Nov 16 22:47:14 2022\n",
      "Finished training epoch 6\n",
      "[xla:0] Accuracy=97.73%\n",
      "[xla:3] Accuracy=97.33%\n",
      "[xla:2] Accuracy=97.52%\n",
      "[xla:1] Accuracy=97.63%\n",
      "[xla:0](0) Loss=0.05950 Rate=70.57 GlobalRate=70.57 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:3](0) Loss=0.04902 Rate=69.88 GlobalRate=69.88 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:2](0) Loss=0.10642 Rate=70.34 GlobalRate=70.34 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:1](0) Loss=0.09928 Rate=70.80 GlobalRate=70.80 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:2](20) Loss=0.07977 Rate=16478.40 GlobalRate=1405.05 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:3](20) Loss=0.03085 Rate=13417.01 GlobalRate=1381.02 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:0](20) Loss=0.04356 Rate=7230.19 GlobalRate=1325.96 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:1](20) Loss=0.02906 Rate=16262.78 GlobalRate=1412.79 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:2](40) Loss=0.08884 Rate=22311.94 GlobalRate=2609.91 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:0](40) Loss=0.11140 Rate=18651.26 GlobalRate=2470.04 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:3](40) Loss=0.11055 Rate=21071.63 GlobalRate=2567.27 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:1](40) Loss=0.07727 Rate=22269.89 GlobalRate=2623.94 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:2](60) Loss=0.10933 Rate=25026.76 GlobalRate=3707.17 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:3](60) Loss=0.05329 Rate=24578.34 GlobalRate=3649.79 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:0](60) Loss=0.03824 Rate=23554.32 GlobalRate=3516.95 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:1](60) Loss=0.07427 Rate=24825.15 GlobalRate=3724.23 Time=Wed Nov 16 22:47:19 2022\n",
      "[xla:3](80) Loss=0.05355 Rate=25678.02 GlobalRate=4636.38 Time=Wed Nov 16 22:47:20 2022[xla:2](80) Loss=0.05315 Rate=25840.17 GlobalRate=4705.83 Time=Wed Nov 16 22:47:20 2022\n",
      "\n",
      "[xla:0](80) Loss=0.04460 Rate=25260.19 GlobalRate=4474.59 Time=Wed Nov 16 22:47:20 2022\n",
      "[xla:1](80) Loss=0.02380 Rate=25895.10 GlobalRate=4728.30 Time=Wed Nov 16 22:47:20 2022\n",
      "[xla:2](100) Loss=0.05397 Rate=25805.59 GlobalRate=5614.72 Time=Wed Nov 16 22:47:20 2022\n",
      "[xla:3](100) Loss=0.02639 Rate=25705.37 GlobalRate=5534.84 Time=Wed Nov 16 22:47:20 2022\n",
      "[xla:0](100) Loss=0.04543 Rate=25572.26 GlobalRate=5350.14 Time=Wed Nov 16 22:47:20 2022\n",
      "[xla:1](100) Loss=0.04921 Rate=26206.24 GlobalRate=5646.21 Time=Wed Nov 16 22:47:20 2022\n",
      "Finished training epoch 7\n",
      "[xla:0] Accuracy=97.96%\n",
      "[xla:1] Accuracy=97.80%\n",
      "[xla:3] Accuracy=97.50%\n",
      "[xla:2] Accuracy=97.65%\n",
      "[xla:0](0) Loss=0.05069 Rate=70.35 GlobalRate=70.35 Time=Wed Nov 16 22:47:24 2022\n",
      "[xla:1](0) Loss=0.08324 Rate=70.89 GlobalRate=70.89 Time=Wed Nov 16 22:47:24 2022\n",
      "[xla:3](0) Loss=0.03745 Rate=67.06 GlobalRate=67.06 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](0) Loss=0.09722 Rate=69.88 GlobalRate=69.88 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:1](20) Loss=0.02248 Rate=6600.05 GlobalRate=1318.12 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:0](20) Loss=0.03465 Rate=4142.12 GlobalRate=1225.79 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:3](20) Loss=0.02700 Rate=16077.50 GlobalRate=1341.10 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](20) Loss=0.06697 Rate=14748.98 GlobalRate=1388.42 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:3](40) Loss=0.10191 Rate=21241.27 GlobalRate=2489.52 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:1](40) Loss=0.07498 Rate=17005.93 GlobalRate=2445.28 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:0](40) Loss=0.09770 Rate=16028.82 GlobalRate=2282.00 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](40) Loss=0.07522 Rate=21385.55 GlobalRate=2578.61 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:1](60) Loss=0.06918 Rate=22718.23 GlobalRate=3481.55 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:0](60) Loss=0.03224 Rate=22342.06 GlobalRate=3258.55 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:3](60) Loss=0.04502 Rate=24385.31 GlobalRate=3541.52 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](60) Loss=0.10149 Rate=23932.46 GlobalRate=3656.99 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:1](80) Loss=0.01958 Rate=22333.33 GlobalRate=4395.75 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:3](80) Loss=0.04656 Rate=23006.98 GlobalRate=4467.80 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:0](80) Loss=0.03720 Rate=22161.96 GlobalRate=4126.89 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](80) Loss=0.04601 Rate=23585.86 GlobalRate=4618.87 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:1](100) Loss=0.04453 Rate=25880.19 GlobalRate=5278.30 Time=Wed Nov 16 22:47:25 2022[xla:3](100) Loss=0.02377 Rate=26147.78 GlobalRate=5361.53 Time=Wed Nov 16 22:47:25 2022\n",
      "\n",
      "[xla:0](100) Loss=0.04158 Rate=25822.08 GlobalRate=4966.80 Time=Wed Nov 16 22:47:25 2022\n",
      "[xla:2](100) Loss=0.04603 Rate=25414.95 GlobalRate=5522.86 Time=Wed Nov 16 22:47:25 2022\n",
      "Finished training epoch 8\n",
      "[xla:0] Accuracy=98.05%\n",
      "[xla:1] Accuracy=97.92%\n",
      "[xla:2] Accuracy=97.78%\n",
      "[xla:3] Accuracy=97.57%\n",
      "[xla:0](0) Loss=0.04613 Rate=72.37 GlobalRate=72.37 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](0) Loss=0.07223 Rate=72.01 GlobalRate=72.01 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:2](0) Loss=0.08897 Rate=70.45 GlobalRate=70.45 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](0) Loss=0.02921 Rate=68.57 GlobalRate=68.57 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:2](20) Loss=0.05700 Rate=10716.15 GlobalRate=1370.99 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](20) Loss=0.01785 Rate=7337.06 GlobalRate=1352.38 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:0](20) Loss=0.02824 Rate=6889.46 GlobalRate=1349.04 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](20) Loss=0.02463 Rate=16959.59 GlobalRate=1373.31 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:0](40) Loss=0.08775 Rate=19197.36 GlobalRate=2515.89 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:2](40) Loss=0.06676 Rate=20663.30 GlobalRate=2554.49 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](40) Loss=0.07086 Rate=18679.92 GlobalRate=2516.83 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](40) Loss=0.08893 Rate=22728.68 GlobalRate=2555.46 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:2](60) Loss=0.09513 Rate=20121.83 GlobalRate=3575.14 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:0](60) Loss=0.02817 Rate=19515.03 GlobalRate=3523.92 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](60) Loss=0.06564 Rate=19664.73 GlobalRate=3531.21 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](60) Loss=0.03868 Rate=20956.11 GlobalRate=3576.55 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](80) Loss=0.01750 Rate=22338.92 GlobalRate=4474.24 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:0](80) Loss=0.03246 Rate=22251.55 GlobalRate=4465.02 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:2](80) Loss=0.03899 Rate=22472.57 GlobalRate=4526.60 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](80) Loss=0.04097 Rate=22647.87 GlobalRate=4525.97 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:1](100) Loss=0.03997 Rate=24168.07 GlobalRate=5346.35 Time=Wed Nov 16 22:47:30 2022[xla:2](100) Loss=0.04056 Rate=24241.44 GlobalRate=5406.57 Time=Wed Nov 16 22:47:30 2022\n",
      "\n",
      "[xla:0](100) Loss=0.03733 Rate=24135.21 GlobalRate=5335.83 Time=Wed Nov 16 22:47:30 2022\n",
      "[xla:3](100) Loss=0.02268 Rate=24268.42 GlobalRate=5405.21 Time=Wed Nov 16 22:47:30 2022\n",
      "Finished training epoch 9\n",
      "[xla:2] Accuracy=97.87%\n",
      "[xla:1] Accuracy=98.06%\n",
      "[xla:0] Accuracy=98.11%\n",
      "[xla:3] Accuracy=97.67%\n",
      "[xla:2](0) Loss=0.08026 Rate=69.73 GlobalRate=69.73 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:1](0) Loss=0.06348 Rate=70.99 GlobalRate=70.99 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:0](0) Loss=0.03991 Rate=71.07 GlobalRate=71.07 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:3](0) Loss=0.02351 Rate=69.01 GlobalRate=69.01 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:0](20) Loss=0.02352 Rate=10456.06 GlobalRate=1379.62 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:1](20) Loss=0.01522 Rate=5725.07 GlobalRate=1296.86 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:2](20) Loss=0.04943 Rate=5568.17 GlobalRate=1272.17 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:3](20) Loss=0.02130 Rate=16523.07 GlobalRate=1379.91 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:1](40) Loss=0.06845 Rate=18615.57 GlobalRate=2422.04 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:2](40) Loss=0.06140 Rate=18557.07 GlobalRate=2377.90 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:0](40) Loss=0.08171 Rate=20453.09 GlobalRate=2569.07 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:3](40) Loss=0.07788 Rate=22873.03 GlobalRate=2569.53 Time=Wed Nov 16 22:47:35 2022\n",
      "[xla:2](60) Loss=0.08901 Rate=24269.85 GlobalRate=3397.50 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:1](60) Loss=0.06310 Rate=24273.71 GlobalRate=3457.84 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:0](60) Loss=0.02527 Rate=25042.58 GlobalRate=3659.10 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:3](60) Loss=0.03421 Rate=25897.21 GlobalRate=3658.67 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:0](80) Loss=0.02792 Rate=26742.63 GlobalRate=4658.32 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:1](80) Loss=0.01596 Rate=26426.23 GlobalRate=4412.03 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:2](80) Loss=0.03612 Rate=26383.15 GlobalRate=4337.57 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:3](80) Loss=0.03701 Rate=26837.59 GlobalRate=4654.92 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:2](100) Loss=0.03646 Rate=26789.93 GlobalRate=5202.67 Time=Wed Nov 16 22:47:36 2022[xla:0](100) Loss=0.03617 Rate=26896.65 GlobalRate=5571.18 Time=Wed Nov 16 22:47:36 2022\n",
      "\n",
      "[xla:1](100) Loss=0.03438 Rate=26766.30 GlobalRate=5288.01 Time=Wed Nov 16 22:47:36 2022\n",
      "[xla:3](100) Loss=0.02131 Rate=27716.34 GlobalRate=5577.77 Time=Wed Nov 16 22:47:36 2022\n",
      "Finished training epoch 10\n",
      "[xla:2] Accuracy=97.96%\n",
      "[xla:1] Accuracy=98.12%\n",
      "[xla:3] Accuracy=97.88%\n",
      "[xla:0] Accuracy=98.18%\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "xmp.spawn(mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "          start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
