{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Colab TPU compatible PyTorch/TPU wheels and dependencies and set PJRT device to be TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nightly PyTorch/XLA\n",
    "# %pip install --user https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torchvision-nightly-cp38-cp38-linux_x86_64.whl 'torch_xla[tpuvm] @ https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly-cp38-cp38-linux_x86_64.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters and Helpers, and start Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result Visualization Helper\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "M, N = 4, 6\n",
    "RESULT_IMG_PATH = '/tmp/test_result.png'\n",
    "\n",
    "def plot_results(images, labels, preds):\n",
    "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
    "  inv_norm = transforms.Normalize((-0.1307/0.3081,), (1/0.3081,))\n",
    "\n",
    "  num_images = images.shape[0]\n",
    "  fig, axes = plt.subplots(M, N, figsize=(11, 9))\n",
    "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
    "\n",
    "  for i, ax in enumerate(fig.axes):\n",
    "    ax.axis('off')\n",
    "    if i >= num_images:\n",
    "      continue\n",
    "    img, label, prediction = images[i], labels[i], preds[i]\n",
    "    img = inv_norm(img)\n",
    "    img = img.squeeze() # [1,Y,X] -> [Y,X]\n",
    "    label, prediction = label.item(), prediction.item()\n",
    "    if label == prediction:\n",
    "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
    "    else:\n",
    "      ax.set_title(\n",
    "          'X {}/{}'.format(label, prediction), color='red')\n",
    "    ax.imshow(img)\n",
    "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "FLAGS['datadir'] = \"/tmp/mnist\"\n",
    "FLAGS['batch_size'] = 128\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 0.01\n",
    "FLAGS['momentum'] = 0.5\n",
    "FLAGS['num_epochs'] = 10\n",
    "FLAGS['num_cores'] = 8\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.utils as xu\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "\n",
    "class MNIST(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(MNIST, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.bn1 = nn.BatchNorm2d(10)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.bn2 = nn.BatchNorm2d(20)\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = self.bn1(x)\n",
    "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "    x = self.bn2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(MNIST())\n",
    "\n",
    "def train_mnist():\n",
    "  torch.manual_seed(1)\n",
    "  \n",
    "  def get_dataset():\n",
    "    norm = transforms.Normalize((0.1307,), (0.3081,))\n",
    "    train_dataset = datasets.MNIST(\n",
    "        FLAGS['datadir'],\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), norm]))\n",
    "    test_dataset = datasets.MNIST(\n",
    "        FLAGS['datadir'],\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), norm]))\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "  \n",
    "  # Using the serial executor avoids multiple processes to\n",
    "  # download the same data.\n",
    "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
    "\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset,\n",
    "    num_replicas=xm.xrt_world_size(),\n",
    "    rank=xm.get_ordinal(),\n",
    "    shuffle=True)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      shuffle=False,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "\n",
    "  # Scale learning rate to world size\n",
    "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "\n",
    "  # Get loss function, optimizer, and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=FLAGS['momentum'])\n",
    "  loss_fn = nn.NLLLoss()\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, (data, target) in enumerate(loader):\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      tracker.add(FLAGS['batch_size'])\n",
    "      if x % FLAGS['log_steps'] == 0:\n",
    "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "            tracker.global_rate(), time.asctime()), flush=True)\n",
    "\n",
    "  def test_loop_fn(loader):\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    data, pred, target = None, None, None\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      pred = output.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      total_samples += data.size()[0]\n",
    "\n",
    "    accuracy = 100.0 * correct / total_samples\n",
    "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
    "        xm.get_ordinal(), accuracy), flush=True)\n",
    "    return accuracy, data, pred, target\n",
    "\n",
    "  # Train and eval loops\n",
    "  accuracy = 0.0\n",
    "  data, pred, target = None, None, None\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)\n",
    "\n",
    "  return accuracy, data, pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  accuracy, data, pred, target = train_mnist()\n",
    "  if rank == 0:\n",
    "    # Retrieve tensors that are on TPU core 0 and plot.\n",
    "    plot_results(data.cpu(), pred.cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xla:3](0) Loss=2.32545 Rate=127.47 GlobalRate=127.47 Time=Tue Nov 22 18:19:19 2022\n",
      "[xla:2](0) Loss=2.34253 Rate=124.34 GlobalRate=124.34 Time=Tue Nov 22 18:19:19 2022\n",
      "[xla:1](0) Loss=2.39215 Rate=127.62 GlobalRate=127.61 Time=Tue Nov 22 18:19:19 2022\n",
      "[xla:0](0) Loss=2.33795 Rate=135.38 GlobalRate=135.37 Time=Tue Nov 22 18:19:19 2022\n",
      "[xla:3](20) Loss=0.80574 Rate=177.09 GlobalRate=203.87 Time=Tue Nov 22 18:19:31 2022[xla:2](20) Loss=0.93456 Rate=176.07 GlobalRate=203.83 Time=Tue Nov 22 18:19:31 2022[xla:0](20) Loss=0.89084 Rate=182.91 GlobalRate=208.78 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "\n",
      "[xla:1](20) Loss=0.83897 Rate=178.97 GlobalRate=206.61 Time=Tue Nov 22 18:19:31 2022\n",
      "[xla:0](40) Loss=0.51583 Rate=19328.44 GlobalRate=405.11 Time=Tue Nov 22 18:19:31 2022[xla:3](40) Loss=0.48324 Rate=19310.42 GlobalRate=395.64 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "[xla:2](40) Loss=0.39709 Rate=18752.48 GlobalRate=395.48 Time=Tue Nov 22 18:19:31 2022\n",
      "[xla:1](40) Loss=0.38117 Rate=17699.59 GlobalRate=400.70 Time=Tue Nov 22 18:19:31 2022\n",
      "[xla:3](60) Loss=0.23352 Rate=19908.68 GlobalRate=583.09 Time=Tue Nov 22 18:19:31 2022[xla:0](60) Loss=0.19538 Rate=19905.14 GlobalRate=596.91 Time=Tue Nov 22 18:19:31 2022[xla:2](60) Loss=0.22936 Rate=19902.58 GlobalRate=582.96 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "\n",
      "[xla:1](60) Loss=0.31042 Rate=18513.77 GlobalRate=590.11 Time=Tue Nov 22 18:19:31 2022\n",
      "[xla:0](80) Loss=0.20755 Rate=22040.93 GlobalRate=786.06 Time=Tue Nov 22 18:19:31 2022[xla:2](80) Loss=0.19693 Rate=22029.52 GlobalRate=767.84 Time=Tue Nov 22 18:19:31 2022[xla:3](80) Loss=0.19329 Rate=22021.85 GlobalRate=768.00 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "\n",
      "[xla:1](80) Loss=0.13979 Rate=22500.18 GlobalRate=777.60 Time=Tue Nov 22 18:19:31 2022\n",
      "[xla:0](100) Loss=0.15626 Rate=26135.30 GlobalRate=973.60 Time=Tue Nov 22 18:19:31 2022[xla:2](100) Loss=0.12156 Rate=26142.94 GlobalRate=951.18 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "[xla:1](100) Loss=0.16952 Rate=26257.95 GlobalRate=963.18 Time=Tue Nov 22 18:19:31 2022[xla:3](100) Loss=0.10701 Rate=24405.97 GlobalRate=950.69 Time=Tue Nov 22 18:19:31 2022\n",
      "\n",
      "Finished training epoch 1\n",
      "[xla:0] Accuracy=96.62%\n",
      "[xla:3] Accuracy=96.62%\n",
      "[xla:1] Accuracy=96.64%\n",
      "[xla:2] Accuracy=96.63%\n",
      "[xla:0](0) Loss=0.11242 Rate=291.46 GlobalRate=291.46 Time=Tue Nov 22 18:19:33 2022\n",
      "[xla:1](0) Loss=0.20037 Rate=268.37 GlobalRate=268.37 Time=Tue Nov 22 18:19:33 2022\n",
      "[xla:3](0) Loss=0.13500 Rate=230.54 GlobalRate=230.54 Time=Tue Nov 22 18:19:33 2022\n",
      "[xla:2](0) Loss=0.13813 Rate=236.90 GlobalRate=236.89 Time=Tue Nov 22 18:19:33 2022\n",
      "[xla:1](20) Loss=0.08919 Rate=9960.42 GlobalRate=4247.47 Time=Tue Nov 22 18:19:34 2022[xla:3](20) Loss=0.06165 Rate=12030.79 GlobalRate=3930.48 Time=Tue Nov 22 18:19:34 2022[xla:0](20) Loss=0.09193 Rate=4373.21 GlobalRate=3359.90 Time=Tue Nov 22 18:19:34 2022\n",
      "\n",
      "\n",
      "[xla:2](20) Loss=0.13188 Rate=18810.79 GlobalRate=4318.76 Time=Tue Nov 22 18:19:34 2022\n",
      "[xla:3](40) Loss=0.16786 Rate=23498.49 GlobalRate=6850.43 Time=Tue Nov 22 18:19:34 2022[xla:0](40) Loss=0.19792 Rate=20498.05 GlobalRate=5950.46 Time=Tue Nov 22 18:19:34 2022\n",
      "\n",
      "[xla:1](40) Loss=0.11492 Rate=22407.28 GlobalRate=7327.33 Time=Tue Nov 22 18:19:34 2022\n",
      "[xla:2](40) Loss=0.11557 Rate=24965.62 GlobalRate=7386.68 Time=Tue Nov 22 18:19:34 2022\n",
      "[xla:0](60) Loss=0.06541 Rate=24663.36 GlobalRate=8006.24 Time=Tue Nov 22 18:19:34 2022[xla:3](60) Loss=0.10246 Rate=25874.61 GlobalRate=9086.32 Time=Tue Nov 22 18:19:34 2022[xla:1](60) Loss=0.15362 Rate=25629.75 GlobalRate=9658.85 Time=Tue Nov 22 18:19:34 2022\n",
      "\n",
      "\n",
      "[xla:2](60) Loss=0.10155 Rate=26635.21 GlobalRate=9726.86 Time=Tue Nov 22 18:19:34 2022\n",
      "[xla:1](80) Loss=0.05844 Rate=26893.90 GlobalRate=11511.37 Time=Tue Nov 22 18:19:34 2022[xla:0](80) Loss=0.11279 Rate=26453.87 GlobalRate=9709.39 Time=Tue Nov 22 18:19:34 2022[xla:3](80) Loss=0.10971 Rate=26944.18 GlobalRate=10892.18 Time=Tue Nov 22 18:19:34 2022\n",
      "\n",
      "\n",
      "[xla:2](80) Loss=0.08600 Rate=27120.79 GlobalRate=11571.37 Time=Tue Nov 22 18:19:34 2022\n",
      "[xla:0](100) Loss=0.08166 Rate=27313.36 GlobalRate=11148.34 Time=Tue Nov 22 18:19:34 2022[xla:1](100) Loss=0.08869 Rate=27500.24 GlobalRate=13026.80 Time=Tue Nov 22 18:19:34 2022[xla:3](100) Loss=0.04404 Rate=27495.16 GlobalRate=12386.05 Time=Tue Nov 22 18:19:34 2022\n",
      "\n",
      "\n",
      "[xla:2](100) Loss=0.05985 Rate=27539.56 GlobalRate=13084.64 Time=Tue Nov 22 18:19:34 2022\n",
      "Finished training epoch 2\n",
      "[xla:0] Accuracy=97.71%[xla:3] Accuracy=97.70%\n",
      "\n",
      "[xla:1] Accuracy=97.68%[xla:2] Accuracy=97.68%\n",
      "\n",
      "[xla:0](0) Loss=0.06434 Rate=328.41 GlobalRate=328.41 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:3](0) Loss=0.08238 Rate=243.58 GlobalRate=243.58 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:1](0) Loss=0.10602 Rate=250.46 GlobalRate=250.46 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:2](0) Loss=0.09034 Rate=241.68 GlobalRate=241.68 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:1](20) Loss=0.04800 Rate=15129.08 GlobalRate=4383.06 Time=Tue Nov 22 18:19:35 2022[xla:3](20) Loss=0.03631 Rate=14610.46 GlobalRate=4257.55 Time=Tue Nov 22 18:19:35 2022[xla:0](20) Loss=0.05832 Rate=13309.37 GlobalRate=5308.89 Time=Tue Nov 22 18:19:35 2022\n",
      "\n",
      "\n",
      "[xla:2](20) Loss=0.09520 Rate=17793.23 GlobalRate=4360.58 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:0](40) Loss=0.13909 Rate=23888.82 GlobalRate=8909.21 Time=Tue Nov 22 18:19:35 2022[xla:3](40) Loss=0.12357 Rate=24400.06 GlobalRate=7348.92 Time=Tue Nov 22 18:19:35 2022\n",
      "\n",
      "[xla:2](40) Loss=0.07662 Rate=25636.47 GlobalRate=7503.85 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:1](40) Loss=0.09314 Rate=23710.79 GlobalRate=7494.45 Time=Tue Nov 22 18:19:35 2022\n",
      "[xla:0](60) Loss=0.04640 Rate=28301.73 GlobalRate=11636.54 Time=Tue Nov 22 18:19:36 2022[xla:3](60) Loss=0.06509 Rate=28452.65 GlobalRate=9805.47 Time=Tue Nov 22 18:19:36 2022\n",
      "\n",
      "[xla:1](60) Loss=0.11970 Rate=28759.53 GlobalRate=10011.06 Time=Tue Nov 22 18:19:36 2022\n",
      "[xla:2](60) Loss=0.07536 Rate=28975.95 GlobalRate=9992.07 Time=Tue Nov 22 18:19:36 2022\n",
      "[xla:0](80) Loss=0.09069 Rate=29115.70 GlobalRate=13690.64 Time=Tue Nov 22 18:19:36 2022[xla:3](80) Loss=0.09345 Rate=29194.22 GlobalRate=11748.19 Time=Tue Nov 22 18:19:36 2022\n",
      "\n",
      "[xla:1](80) Loss=0.03604 Rate=28496.71 GlobalRate=11912.74 Time=Tue Nov 22 18:19:36 2022\n",
      "[xla:2](80) Loss=0.06064 Rate=27936.51 GlobalRate=11843.90 Time=Tue Nov 22 18:19:36 2022\n",
      "[xla:0](100) Loss=0.04835 Rate=27844.25 GlobalRate=15171.33 Time=Tue Nov 22 18:19:36 2022[xla:3](100) Loss=0.03083 Rate=27837.51 GlobalRate=13224.61 Time=Tue Nov 22 18:19:36 2022[xla:1](100) Loss=0.06014 Rate=28469.48 GlobalRate=13462.34 Time=Tue Nov 22 18:19:36 2022\n",
      "\n",
      "\n",
      "[xla:2](100) Loss=0.04202 Rate=27828.82 GlobalRate=13360.68 Time=Tue Nov 22 18:19:36 2022\n",
      "Finished training epoch 3\n",
      "[xla:0] Accuracy=98.08%\n",
      "[xla:2] Accuracy=98.11%\n",
      "[xla:1] Accuracy=98.12%\n",
      "[xla:3] Accuracy=98.11%\n",
      "[xla:0](0) Loss=0.04767 Rate=240.76 GlobalRate=240.75 Time=Tue Nov 22 18:19:37 2022\n",
      "[xla:1](0) Loss=0.07479 Rate=250.51 GlobalRate=250.50 Time=Tue Nov 22 18:19:37 2022\n",
      "[xla:2](0) Loss=0.07453 Rate=243.18 GlobalRate=243.17 Time=Tue Nov 22 18:19:37 2022\n",
      "[xla:3](0) Loss=0.05711 Rate=232.96 GlobalRate=232.95 Time=Tue Nov 22 18:19:37 2022\n",
      "[xla:2](20) Loss=0.07248 Rate=9162.64 GlobalRate=3863.14 Time=Tue Nov 22 18:19:37 2022[xla:1](20) Loss=0.03411 Rate=8536.60 GlobalRate=3878.57 Time=Tue Nov 22 18:19:37 2022[xla:0](20) Loss=0.03922 Rate=7717.12 GlobalRate=3666.01 Time=Tue Nov 22 18:19:37 2022\n",
      "\n",
      "\n",
      "[xla:3](20) Loss=0.02722 Rate=17919.65 GlobalRate=4228.88 Time=Tue Nov 22 18:19:37 2022\n",
      "[xla:1](40) Loss=0.08120 Rate=22136.38 GlobalRate=6770.90 Time=Tue Nov 22 18:19:37 2022[xla:2](40) Loss=0.05477 Rate=22383.10 GlobalRate=6746.67 Time=Tue Nov 22 18:19:37 2022\n",
      "\n",
      "[xla:0](40) Loss=0.10599 Rate=20933.02 GlobalRate=6405.51 Time=Tue Nov 22 18:19:37 2022[xla:3](40) Loss=0.10301 Rate=25994.15 GlobalRate=7317.19 Time=Tue Nov 22 18:19:37 2022\n",
      "\n",
      "[xla:2](60) Loss=0.06116 Rate=25241.88 GlobalRate=8952.43 Time=Tue Nov 22 18:19:37 2022[xla:1](60) Loss=0.10133 Rate=25144.10 GlobalRate=8981.15 Time=Tue Nov 22 18:19:37 2022[xla:0](60) Loss=0.03712 Rate=25378.76 GlobalRate=8583.86 Time=Tue Nov 22 18:19:37 2022\n",
      "\n",
      "\n",
      "[xla:3](60) Loss=0.04263 Rate=25811.81 GlobalRate=9558.48 Time=Tue Nov 22 18:19:38 2022\n",
      "[xla:1](80) Loss=0.02516 Rate=25120.23 GlobalRate=10673.79 Time=Tue Nov 22 18:19:38 2022[xla:2](80) Loss=0.05116 Rate=25165.16 GlobalRate=10643.68 Time=Tue Nov 22 18:19:38 2022[xla:0](80) Loss=0.07940 Rate=25205.16 GlobalRate=10248.56 Time=Tue Nov 22 18:19:38 2022\n",
      "\n",
      "\n",
      "[xla:3](80) Loss=0.08123 Rate=25427.21 GlobalRate=11287.08 Time=Tue Nov 22 18:19:38 2022\n",
      "[xla:0](100) Loss=0.03230 Rate=25815.77 GlobalRate=11654.45 Time=Tue Nov 22 18:19:38 2022[xla:1](100) Loss=0.04774 Rate=25751.85 GlobalRate=12091.70 Time=Tue Nov 22 18:19:38 2022[xla:2](100) Loss=0.03453 Rate=25762.17 GlobalRate=12060.17 Time=Tue Nov 22 18:19:38 2022\n",
      "\n",
      "\n",
      "[xla:3](100) Loss=0.02424 Rate=25751.07 GlobalRate=12709.90 Time=Tue Nov 22 18:19:38 2022\n",
      "Finished training epoch 4\n",
      "[xla:1] Accuracy=98.28%\n",
      "[xla:2] Accuracy=98.29%\n",
      "[xla:0] Accuracy=98.30%\n",
      "[xla:3] Accuracy=98.31%\n",
      "[xla:1](0) Loss=0.05798 Rate=244.77 GlobalRate=244.76 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:2](0) Loss=0.06590 Rate=233.25 GlobalRate=233.25 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:3](0) Loss=0.03852 Rate=229.29 GlobalRate=229.29 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:0](0) Loss=0.03830 Rate=188.70 GlobalRate=188.70 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:2](20) Loss=0.05677 Rate=6874.35 GlobalRate=3467.13 Time=Tue Nov 22 18:19:39 2022[xla:3](20) Loss=0.01983 Rate=11406.09 GlobalRate=3873.13 Time=Tue Nov 22 18:19:39 2022[xla:1](20) Loss=0.02546 Rate=5809.89 GlobalRate=3394.50 Time=Tue Nov 22 18:19:39 2022[xla:0](20) Loss=0.03033 Rate=22233.30 GlobalRate=3595.30 Time=Tue Nov 22 18:19:39 2022\n",
      "\n",
      "\n",
      "\n",
      "[xla:3](40) Loss=0.08667 Rate=22762.32 GlobalRate=6741.98 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:1](40) Loss=0.07251 Rate=20057.39 GlobalRate=5973.93 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:2](40) Loss=0.04284 Rate=20100.11 GlobalRate=6075.43 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:0](40) Loss=0.08516 Rate=26092.22 GlobalRate=6270.39 Time=Tue Nov 22 18:19:39 2022\n",
      "[xla:2](60) Loss=0.05094 Rate=24398.25 GlobalRate=8152.85 Time=Tue Nov 22 18:19:40 2022[xla:3](60) Loss=0.03008 Rate=24765.82 GlobalRate=8908.23 Time=Tue Nov 22 18:19:40 2022\n",
      "\n",
      "[xla:0](60) Loss=0.03007 Rate=26554.22 GlobalRate=8375.42 Time=Tue Nov 22 18:19:40 2022[xla:1](60) Loss=0.08848 Rate=23560.23 GlobalRate=7989.02 Time=Tue Nov 22 18:19:40 2022\n",
      "\n",
      "[xla:3](80) Loss=0.07110 Rate=25683.08 GlobalRate=10646.50 Time=Tue Nov 22 18:19:40 2022[xla:0](80) Loss=0.07056 Rate=26921.79 GlobalRate=10100.49 Time=Tue Nov 22 18:19:40 2022[xla:2](80) Loss=0.04526 Rate=25526.59 GlobalRate=9826.38 Time=Tue Nov 22 18:19:40 2022\n",
      "\n",
      "\n",
      "[xla:1](80) Loss=0.01967 Rate=24944.37 GlobalRate=9632.89 Time=Tue Nov 22 18:19:40 2022\n",
      "[xla:3](100) Loss=0.01971 Rate=27967.75 GlobalRate=12188.78 Time=Tue Nov 22 18:19:40 2022[xla:2](100) Loss=0.03086 Rate=27819.94 GlobalRate=11317.08 Time=Tue Nov 22 18:19:40 2022[xla:0](100) Loss=0.02306 Rate=28297.29 GlobalRate=11603.84 Time=Tue Nov 22 18:19:40 2022\n",
      "\n",
      "\n",
      "[xla:1](100) Loss=0.03887 Rate=28202.35 GlobalRate=11139.16 Time=Tue Nov 22 18:19:40 2022\n",
      "Finished training epoch 5\n",
      "[xla:1] Accuracy=98.46%\n",
      "[xla:2] Accuracy=98.46%\n",
      "[xla:0] Accuracy=98.45%\n",
      "[xla:3] Accuracy=98.47%\n",
      "[xla:1](0) Loss=0.04697 Rate=232.44 GlobalRate=232.44 Time=Tue Nov 22 18:19:41 2022\n",
      "[xla:0](0) Loss=0.03369 Rate=240.51 GlobalRate=240.51 Time=Tue Nov 22 18:19:41 2022\n",
      "[xla:2](0) Loss=0.05627 Rate=263.74 GlobalRate=263.74 Time=Tue Nov 22 18:19:41 2022\n",
      "[xla:3](0) Loss=0.02885 Rate=181.46 GlobalRate=181.46 Time=Tue Nov 22 18:19:41 2022\n",
      "[xla:0](20) Loss=0.02579 Rate=5253.79 GlobalRate=3238.49 Time=Tue Nov 22 18:19:41 2022[xla:2](20) Loss=0.04665 Rate=5633.19 GlobalRate=3522.00 Time=Tue Nov 22 18:19:41 2022[xla:1](20) Loss=0.02166 Rate=5064.54 GlobalRate=3126.88 Time=Tue Nov 22 18:19:41 2022\n",
      "\n",
      "\n",
      "[xla:3](20) Loss=0.01553 Rate=18267.69 GlobalRate=3403.35 Time=Tue Nov 22 18:19:41 2022\n",
      "[xla:3](40) Loss=0.07676 Rate=24557.37 GlobalRate=5971.44 Time=Tue Nov 22 18:19:42 2022[xla:2](40) Loss=0.03343 Rate=19241.16 GlobalRate=6147.93 Time=Tue Nov 22 18:19:42 2022[xla:0](40) Loss=0.07267 Rate=19050.20 GlobalRate=5700.37 Time=Tue Nov 22 18:19:42 2022\n",
      "\n",
      "\n",
      "[xla:1](40) Loss=0.06681 Rate=18504.70 GlobalRate=5507.69 Time=Tue Nov 22 18:19:42 2022\n",
      "[xla:3](60) Loss=0.02291 Rate=28102.78 GlobalRate=8109.01 Time=Tue Nov 22 18:19:42 2022[xla:0](60) Loss=0.02529 Rate=25932.64 GlobalRate=7772.87 Time=Tue Nov 22 18:19:42 2022[xla:2](60) Loss=0.04313 Rate=25948.12 GlobalRate=8326.07 Time=Tue Nov 22 18:19:42 2022\n",
      "\n",
      "\n",
      "[xla:1](60) Loss=0.07674 Rate=25628.54 GlobalRate=7528.53 Time=Tue Nov 22 18:19:42 2022\n",
      "[xla:3](80) Loss=0.06395 Rate=29140.09 GlobalRate=9886.58 Time=Tue Nov 22 18:19:42 2022[xla:0](80) Loss=0.06337 Rate=28247.26 GlobalRate=9507.98 Time=Tue Nov 22 18:19:42 2022[xla:2](80) Loss=0.04199 Rate=28106.14 GlobalRate=10120.79 Time=Tue Nov 22 18:19:42 2022\n",
      "\n",
      "\n",
      "[xla:1](80) Loss=0.01674 Rate=28086.33 GlobalRate=9230.41 Time=Tue Nov 22 18:19:42 2022\n",
      "[xla:0](100) Loss=0.01796 Rate=28973.13 GlobalRate=10980.50 Time=Tue Nov 22 18:19:42 2022[xla:2](100) Loss=0.02736 Rate=29058.33 GlobalRate=11640.15 Time=Tue Nov 22 18:19:42 2022[xla:3](100) Loss=0.01665 Rate=29316.07 GlobalRate=11383.59 Time=Tue Nov 22 18:19:42 2022\n",
      "\n",
      "\n",
      "[xla:1](100) Loss=0.03223 Rate=28507.54 GlobalRate=10665.17 Time=Tue Nov 22 18:19:42 2022\n",
      "Finished training epoch 6\n",
      "[xla:2] Accuracy=98.55%\n",
      "[xla:0] Accuracy=98.57%\n",
      "[xla:1] Accuracy=98.56%\n",
      "[xla:3] Accuracy=98.58%\n",
      "[xla:0](0) Loss=0.03009 Rate=269.98 GlobalRate=269.97 Time=Tue Nov 22 18:19:43 2022\n",
      "[xla:2](0) Loss=0.04880 Rate=263.85 GlobalRate=263.85 Time=Tue Nov 22 18:19:43 2022\n",
      "[xla:1](0) Loss=0.04059 Rate=257.59 GlobalRate=257.59 Time=Tue Nov 22 18:19:43 2022\n",
      "[xla:3](0) Loss=0.02248 Rate=193.13 GlobalRate=193.12 Time=Tue Nov 22 18:19:43 2022\n",
      "[xla:0](20) Loss=0.02231 Rate=4197.53 GlobalRate=3163.41 Time=Tue Nov 22 18:19:43 2022[xla:1](20) Loss=0.01836 Rate=4660.60 GlobalRate=3223.26 Time=Tue Nov 22 18:19:43 2022[xla:2](20) Loss=0.03843 Rate=4281.53 GlobalRate=3151.41 Time=Tue Nov 22 18:19:43 2022\n",
      "[xla:3](20) Loss=0.01254 Rate=17173.69 GlobalRate=3571.44 Time=Tue Nov 22 18:19:43 2022\n",
      "\n",
      "\n",
      "[xla:0](40) Loss=0.05977 Rate=19526.22 GlobalRate=5608.19 Time=Tue Nov 22 18:19:44 2022[xla:3](40) Loss=0.06732 Rate=24933.32 GlobalRate=6265.07 Time=Tue Nov 22 18:19:44 2022[xla:2](40) Loss=0.02667 Rate=19382.28 GlobalRate=5583.72 Time=Tue Nov 22 18:19:44 2022\n",
      "\n",
      "\n",
      "[xla:1](40) Loss=0.06368 Rate=19369.19 GlobalRate=5693.91 Time=Tue Nov 22 18:19:44 2022\n",
      "[xla:1](60) Loss=0.06898 Rate=27588.55 GlobalRate=7815.03 Time=Tue Nov 22 18:19:44 2022[xla:3](60) Loss=0.01968 Rate=29310.85 GlobalRate=8513.88 Time=Tue Nov 22 18:19:44 2022\n",
      "\n",
      "[xla:2](60) Loss=0.03668 Rate=26821.82 GlobalRate=7651.69 Time=Tue Nov 22 18:19:44 2022\n",
      "[xla:0](60) Loss=0.02353 Rate=26178.87 GlobalRate=7659.43 Time=Tue Nov 22 18:19:44 2022\n",
      "[xla:1](80) Loss=0.01552 Rate=30239.96 GlobalRate=9608.16 Time=Tue Nov 22 18:19:44 2022\n",
      "[xla:3](80) Loss=0.05898 Rate=30733.84 GlobalRate=10389.88 Time=Tue Nov 22 18:19:44 2022[xla:0](80) Loss=0.05817 Rate=30321.28 GlobalRate=9453.14 Time=Tue Nov 22 18:19:44 2022\n",
      "\n",
      "[xla:2](80) Loss=0.03835 Rate=29654.29 GlobalRate=9411.87 Time=Tue Nov 22 18:19:44 2022\n",
      "[xla:2](100) Loss=0.02576 Rate=30115.29 GlobalRate=10902.96 Time=Tue Nov 22 18:19:44 2022[xla:0](100) Loss=0.01451 Rate=30001.22 GlobalRate=10930.73 Time=Tue Nov 22 18:19:44 2022[xla:1](100) Loss=0.02655 Rate=29604.21 GlobalRate=11079.75 Time=Tue Nov 22 18:19:44 2022\n",
      "\n",
      "\n",
      "[xla:3](100) Loss=0.01345 Rate=28621.64 GlobalRate=11839.21 Time=Tue Nov 22 18:19:44 2022\n",
      "Finished training epoch 7\n",
      "[xla:0] Accuracy=98.62%\n",
      "[xla:1] Accuracy=98.63%\n",
      "[xla:2] Accuracy=98.62%\n",
      "[xla:3] Accuracy=98.62%\n",
      "[xla:0](0) Loss=0.02651 Rate=233.04 GlobalRate=233.04 Time=Tue Nov 22 18:19:45 2022\n",
      "[xla:1](0) Loss=0.03603 Rate=243.00 GlobalRate=243.00 Time=Tue Nov 22 18:19:45 2022\n",
      "[xla:2](0) Loss=0.04404 Rate=234.95 GlobalRate=234.94 Time=Tue Nov 22 18:19:45 2022\n",
      "[xla:3](0) Loss=0.01818 Rate=196.13 GlobalRate=196.12 Time=Tue Nov 22 18:19:45 2022\n",
      "[xla:1](20) Loss=0.01632 Rate=5604.24 GlobalRate=3336.32 Time=Tue Nov 22 18:19:45 2022[xla:2](20) Loss=0.03376 Rate=6531.76 GlobalRate=3431.16 Time=Tue Nov 22 18:19:45 2022[xla:0](20) Loss=0.01924 Rate=5471.05 GlobalRate=3219.61 Time=Tue Nov 22 18:19:45 2022\n",
      "\n",
      "\n",
      "[xla:3](20) Loss=0.01078 Rate=18950.29 GlobalRate=3661.92 Time=Tue Nov 22 18:19:45 2022\n",
      "[xla:1](40) Loss=0.05987 Rate=20454.19 GlobalRate=5896.52 Time=Tue Nov 22 18:19:46 2022[xla:0](40) Loss=0.04968 Rate=20409.41 GlobalRate=5709.45 Time=Tue Nov 22 18:19:46 2022[xla:3](40) Loss=0.06199 Rate=26245.29 GlobalRate=6428.73 Time=Tue Nov 22 18:19:46 2022\n",
      "\n",
      "\n",
      "[xla:2](40) Loss=0.02141 Rate=20186.41 GlobalRate=6026.59 Time=Tue Nov 22 18:19:46 2022\n",
      "[xla:1](60) Loss=0.06205 Rate=25179.28 GlobalRate=7964.24 Time=Tue Nov 22 18:19:46 2022[xla:2](60) Loss=0.03047 Rate=25667.32 GlobalRate=8149.29 Time=Tue Nov 22 18:19:46 2022[xla:0](60) Loss=0.02269 Rate=25140.10 GlobalRate=7733.31 Time=Tue Nov 22 18:19:46 2022\n",
      "\n",
      "\n",
      "[xla:3](60) Loss=0.01710 Rate=26927.74 GlobalRate=8581.88 Time=Tue Nov 22 18:19:46 2022\n",
      "[xla:1](80) Loss=0.01485 Rate=26441.73 GlobalRate=9651.70 Time=Tue Nov 22 18:19:46 2022[xla:2](80) Loss=0.03606 Rate=26639.44 GlobalRate=9856.12 Time=Tue Nov 22 18:19:46 2022[xla:0](80) Loss=0.05433 Rate=26428.05 GlobalRate=9395.76 Time=Tue Nov 22 18:19:46 2022\n",
      "\n",
      "\n",
      "[xla:3](80) Loss=0.05287 Rate=26979.50 GlobalRate=10320.57 Time=Tue Nov 22 18:19:46 2022\n",
      "[xla:1](100) Loss=0.02326 Rate=26660.34 GlobalRate=11052.30 Time=Tue Nov 22 18:19:46 2022[xla:2](100) Loss=0.02391 Rate=26748.59 GlobalRate=11267.39 Time=Tue Nov 22 18:19:46 2022[xla:0](100) Loss=0.01245 Rate=26637.14 GlobalRate=10781.58 Time=Tue Nov 22 18:19:46 2022\n",
      "\n",
      "\n",
      "[xla:3](100) Loss=0.01158 Rate=27146.68 GlobalRate=11768.65 Time=Tue Nov 22 18:19:46 2022\n",
      "Finished training epoch 8\n",
      "[xla:1] Accuracy=98.65%\n",
      "[xla:0] Accuracy=98.63%\n",
      "[xla:2] Accuracy=98.66%\n",
      "[xla:3] Accuracy=98.66%\n",
      "[xla:0](0) Loss=0.02327 Rate=234.07 GlobalRate=234.06 Time=Tue Nov 22 18:19:47 2022\n",
      "[xla:1](0) Loss=0.03035 Rate=227.67 GlobalRate=227.66 Time=Tue Nov 22 18:19:47 2022\n",
      "[xla:2](0) Loss=0.04115 Rate=225.94 GlobalRate=225.94 Time=Tue Nov 22 18:19:47 2022\n",
      "[xla:3](0) Loss=0.01480 Rate=224.96 GlobalRate=224.95 Time=Tue Nov 22 18:19:47 2022\n",
      "[xla:0](20) Loss=0.01747 Rate=9040.66 GlobalRate=3740.93 Time=Tue Nov 22 18:19:47 2022[xla:2](20) Loss=0.02965 Rate=17250.34 GlobalRate=4097.30 Time=Tue Nov 22 18:19:47 2022[xla:1](20) Loss=0.01462 Rate=9613.49 GlobalRate=3715.07 Time=Tue Nov 22 18:19:47 2022\n",
      "\n",
      "\n",
      "[xla:3](20) Loss=0.00942 Rate=18468.05 GlobalRate=4119.03 Time=Tue Nov 22 18:19:47 2022\n",
      "[xla:2](40) Loss=0.01889 Rate=26296.87 GlobalRate=7137.92 Time=Tue Nov 22 18:19:47 2022[xla:1](40) Loss=0.05487 Rate=23293.48 GlobalRate=6539.43 Time=Tue Nov 22 18:19:47 2022\n",
      "\n",
      "[xla:0](40) Loss=0.04099 Rate=22462.10 GlobalRate=6559.61 Time=Tue Nov 22 18:19:48 2022[xla:3](40) Loss=0.05681 Rate=26720.57 GlobalRate=7169.10 Time=Tue Nov 22 18:19:48 2022\n",
      "\n",
      "[xla:2](60) Loss=0.02725 Rate=29649.38 GlobalRate=9574.27 Time=Tue Nov 22 18:19:48 2022[xla:1](60) Loss=0.05443 Rate=28334.01 GlobalRate=8839.68 Time=Tue Nov 22 18:19:48 2022[xla:0](60) Loss=0.02034 Rate=28596.56 GlobalRate=8889.28 Time=Tue Nov 22 18:19:48 2022\n",
      "\n",
      "\n",
      "[xla:3](60) Loss=0.01538 Rate=28687.57 GlobalRate=9552.64 Time=Tue Nov 22 18:19:48 2022\n",
      "[xla:0](80) Loss=0.04723 Rate=28029.92 GlobalRate=10678.31 Time=Tue Nov 22 18:19:48 2022[xla:1](80) Loss=0.01204 Rate=27891.86 GlobalRate=10622.38 Time=Tue Nov 22 18:19:48 2022[xla:2](80) Loss=0.03605 Rate=28355.47 GlobalRate=11410.54 Time=Tue Nov 22 18:19:48 2022\n",
      "\n",
      "\n",
      "[xla:3](80) Loss=0.04931 Rate=27844.58 GlobalRate=11378.40 Time=Tue Nov 22 18:19:48 2022\n",
      "[xla:1](100) Loss=0.02025 Rate=27706.91 GlobalRate=12095.13 Time=Tue Nov 22 18:19:48 2022[xla:0](100) Loss=0.01080 Rate=27737.05 GlobalRate=12151.63 Time=Tue Nov 22 18:19:48 2022[xla:2](100) Loss=0.02176 Rate=27906.57 GlobalRate=12910.41 Time=Tue Nov 22 18:19:48 2022\n",
      "\n",
      "\n",
      "[xla:3](100) Loss=0.01018 Rate=27887.44 GlobalRate=12890.56 Time=Tue Nov 22 18:19:48 2022\n",
      "Finished training epoch 9\n",
      "[xla:0] Accuracy=98.69%\n",
      "[xla:2] Accuracy=98.70%\n",
      "[xla:1] Accuracy=98.73%\n",
      "[xla:3] Accuracy=98.71%\n",
      "[xla:2](0) Loss=0.03742 Rate=242.79 GlobalRate=242.79 Time=Tue Nov 22 18:19:49 2022[xla:0](0) Loss=0.02073 Rate=233.44 GlobalRate=233.44 Time=Tue Nov 22 18:19:49 2022\n",
      "\n",
      "[xla:1](0) Loss=0.02620 Rate=236.05 GlobalRate=236.05 Time=Tue Nov 22 18:19:49 2022\n",
      "[xla:3](0) Loss=0.01206 Rate=192.38 GlobalRate=192.38 Time=Tue Nov 22 18:19:49 2022\n",
      "[xla:2](20) Loss=0.02622 Rate=5263.45 GlobalRate=3260.07 Time=Tue Nov 22 18:19:49 2022[xla:1](20) Loss=0.01363 Rate=6206.08 GlobalRate=3387.11 Time=Tue Nov 22 18:19:49 2022[xla:0](20) Loss=0.01744 Rate=5321.76 GlobalRate=3191.96 Time=Tue Nov 22 18:19:49 2022\n",
      "\n",
      "\n",
      "[xla:3](20) Loss=0.00853 Rate=16867.59 GlobalRate=3551.58 Time=Tue Nov 22 18:19:49 2022\n",
      "[xla:2](40) Loss=0.01815 Rate=16348.06 GlobalRate=5628.70 Time=Tue Nov 22 18:19:50 2022[xla:1](40) Loss=0.04914 Rate=16730.28 GlobalRate=5822.10 Time=Tue Nov 22 18:19:50 2022[xla:0](40) Loss=0.03457 Rate=16378.57 GlobalRate=5524.79 Time=Tue Nov 22 18:19:50 2022\n",
      "\n",
      "\n",
      "[xla:3](40) Loss=0.05314 Rate=21070.69 GlobalRate=6073.50 Time=Tue Nov 22 18:19:50 2022\n",
      "[xla:0](60) Loss=0.01843 Rate=25035.16 GlobalRate=7558.56 Time=Tue Nov 22 18:19:50 2022\n",
      "[xla:1](60) Loss=0.05210 Rate=24637.21 GlobalRate=7910.95 Time=Tue Nov 22 18:19:50 2022[xla:2](60) Loss=0.02470 Rate=24466.89 GlobalRate=7669.61 Time=Tue Nov 22 18:19:50 2022[xla:3](60) Loss=0.01388 Rate=27106.54 GlobalRate=8250.94 Time=Tue Nov 22 18:19:50 2022\n",
      "\n",
      "\n",
      "[xla:0](80) Loss=0.04200 Rate=28397.28 GlobalRate=9285.70 Time=Tue Nov 22 18:19:50 2022[xla:1](80) Loss=0.01113 Rate=28816.26 GlobalRate=9707.93 Time=Tue Nov 22 18:19:50 2022[xla:2](80) Loss=0.03341 Rate=28544.42 GlobalRate=9426.07 Time=Tue Nov 22 18:19:50 2022\n",
      "\n",
      "[xla:3](80) Loss=0.04337 Rate=29384.39 GlobalRate=10074.28 Time=Tue Nov 22 18:19:50 2022\n",
      "\n",
      "[xla:2](100) Loss=0.01985 Rate=29307.25 GlobalRate=10902.43 Time=Tue Nov 22 18:19:50 2022[xla:0](100) Loss=0.00997 Rate=29084.98 GlobalRate=10744.60 Time=Tue Nov 22 18:19:50 2022[xla:1](100) Loss=0.01830 Rate=29231.04 GlobalRate=11195.48 Time=Tue Nov 22 18:19:50 2022\n",
      "\n",
      "\n",
      "[xla:3](100) Loss=0.00881 Rate=29257.56 GlobalRate=11574.82 Time=Tue Nov 22 18:19:50 2022\n",
      "Finished training epoch 10\n",
      "[xla:1] Accuracy=98.75%\n",
      "[xla:0] Accuracy=98.77%\n",
      "[xla:3] Accuracy=98.75%\n",
      "[xla:2] Accuracy=98.76%\n"
     ]
    }
   ],
   "source": [
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
